{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efficient Embeddings Generation \n",
    "\n",
    "In this video, we'll fous on genearting embeddings efficiently using 2 strategies:\n",
    "\n",
    "- Batching: Processing multiple inputs together rather than one at a time\n",
    "- Caching: Storing previously generated embeddings to avoid regeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required packages\n",
    "!uv pip install accelerate==1.6.0 sentence-transformers==4.0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import hashlib\n",
    "from functools import lru_cache\n",
    "\n",
    "# Load model\n",
    "model_name = 'all-MiniLM-L6-v2'\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "# Generate example sentences for benchmarking\n",
    "sentences = [\n",
    "    f\"This is a sample sentence for benchmarking embeddings generation {i}.\"\n",
    "    for i in range(1000)\n",
    "]\n",
    "\n",
    "print(f\"Created {len(sentences)} example sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Batch Size Impact on Throughput and Latency\n",
    "import pandas as pd\n",
    "batch_sizes = [1, 4, 8, 16, 32, 64, 128, 256]\n",
    "results = []\n",
    "\n",
    "print(\"Measuring impact of batch size on throughput and latency...\")\n",
    "\n",
    "for batch_size in tqdm(batch_sizes):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Process data in batches\n",
    "    embeddings = []\n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "        batch = sentences[i:i+batch_size]\n",
    "        batch_embeddings = model.encode(batch)\n",
    "        embeddings.extend(batch_embeddings)\n",
    "\n",
    "    # Calculate metrics\n",
    "    total_time = time.time() - start_time\n",
    "    throughput = len(sentences) / total_time\n",
    "    avg_latency = total_time / (len(sentences) / batch_size)\n",
    "\n",
    "    results.append({\n",
    "        'Batch Size': batch_size,\n",
    "        'Total Time (s)': total_time,\n",
    "        'Throughput (samples/s)': throughput,\n",
    "        'Avg Batch Latency (s)': avg_latency\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(df['Batch Size'], df['Throughput (samples/s)'], 'o-', linewidth=2)\n",
    "plt.xlabel('Batch Size')\n",
    "plt.ylabel('Throughput (samples/s)')\n",
    "plt.title('Batch Size vs Throughput')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(df['Batch Size'], df['Avg Batch Latency (s)'], 'o-', linewidth=2)\n",
    "plt.xlabel('Batch Size')\n",
    "plt.ylabel('Average Batch Latency (s)')\n",
    "plt.title('Batch Size vs Latency')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Implementing a Simple Embedding Cache\n",
    "\n",
    "class SimpleEmbeddingCache:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.cache = {}\n",
    "        self.hits = 0\n",
    "        self.misses = 0\n",
    "\n",
    "    def _get_hash(self, text):\n",
    "        return hashlib.md5(text.encode('utf-8')).hexdigest()\n",
    "\n",
    "    def encode(self, texts, batch_size=32):\n",
    "        results = []\n",
    "        texts_to_encode = []\n",
    "        text_indices = []\n",
    "\n",
    "        # Check cache for each text\n",
    "        for i, text in enumerate(texts):\n",
    "            text_hash = self._get_hash(text)\n",
    "            if text_hash in self.cache:\n",
    "                results.append((i, self.cache[text_hash]))\n",
    "                self.hits += 1\n",
    "            else:\n",
    "                texts_to_encode.append(text)\n",
    "                text_indices.append(i)\n",
    "                self.misses += 1\n",
    "\n",
    "        # Generate embeddings for cache misses\n",
    "        if texts_to_encode:\n",
    "            # Process in batches\n",
    "            new_embeddings = []\n",
    "            for i in range(0, len(texts_to_encode), batch_size):\n",
    "                batch = texts_to_encode[i:i+batch_size]\n",
    "                batch_embeddings = self.model.encode(batch)\n",
    "                new_embeddings.extend(batch_embeddings)\n",
    "\n",
    "            # Update cache with new embeddings\n",
    "            for i, text in enumerate(texts_to_encode):\n",
    "                text_hash = self._get_hash(text)\n",
    "                self.cache[text_hash] = new_embeddings[i]\n",
    "                results.append((text_indices[i], new_embeddings[i]))\n",
    "\n",
    "        # Sort by original index and extract embeddings\n",
    "        results.sort(key=lambda x: x[0])\n",
    "        return np.array([emb for _, emb in results])\n",
    "\n",
    "    def get_stats(self):\n",
    "        total = self.hits + self.misses\n",
    "        hit_rate = self.hits / total if total > 0 else 0\n",
    "        return {\n",
    "            \"hits\": self.hits,\n",
    "            \"misses\": self.misses,\n",
    "            \"total\": total,\n",
    "            \"hit_rate\": hit_rate\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b. Function-based cache using lru_cache\n",
    "@lru_cache(maxsize=1024)\n",
    "def hash_text(text):\n",
    "    return hashlib.md5(text.encode('utf-8')).hexdigest()\n",
    "\n",
    "\n",
    "class LRUEmbeddingCache:\n",
    "    def __init__(self, model, maxsize=1024):\n",
    "        self.model = model\n",
    "        self.encode_single = lru_cache(maxsize=maxsize)(self._encode_single)\n",
    "        self.hits = 0\n",
    "        self.misses = 0\n",
    "\n",
    "    def _encode_single(self, text_hash):\n",
    "        self.misses += 1\n",
    "        # Convert hash back to the original text\n",
    "        text = self.hash_to_text[text_hash]\n",
    "        return self.model.encode(text)\n",
    "\n",
    "    def encode(self, texts, batch_size=32):\n",
    "        self.hash_to_text = {}\n",
    "        results = []\n",
    "\n",
    "        for text in texts:\n",
    "            # Use the hash as the cache key\n",
    "            text_hash = hash_text(text)\n",
    "            self.hash_to_text[text_hash] = text\n",
    "\n",
    "            # Get from cache if exists\n",
    "            embedding = self.encode_single(text_hash)\n",
    "            if embedding is not None:\n",
    "                results.append(embedding)\n",
    "\n",
    "        return np.array(results)\n",
    "\n",
    "    def get_stats(self):\n",
    "        total = self.hits + self.misses\n",
    "        hit_rate = self.hits / total if total > 0 else 0\n",
    "        return {\n",
    "            \"hits\": self.hits,\n",
    "            \"misses\": self.misses,\n",
    "            \"total\": total,\n",
    "            \"hit_rate\": hit_rate\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark caching strategies\n",
    "print(\"\\nBenchmarking caching strategies...\")\n",
    "\n",
    "# Generate a dataset with some repeated content\n",
    "repeated_sentences = []\n",
    "for i in range(500):\n",
    "    # Add some repetition to demonstrate cache benefits\n",
    "    if i % 5 == 0:\n",
    "        repeated_sentences.append(sentences[i % 100])\n",
    "    else:\n",
    "        repeated_sentences.append(sentences[i])\n",
    "\n",
    "# Test without cache\n",
    "start_time = time.time()\n",
    "embeddings_no_cache = model.encode(repeated_sentences, batch_size=32)\n",
    "no_cache_time = time.time() - start_time\n",
    "print(f\"Time without cache: {no_cache_time:.4f}s\")\n",
    "\n",
    "# Test with simple cache\n",
    "simple_cache = SimpleEmbeddingCache(model)\n",
    "start_time = time.time()\n",
    "embeddings_simple_cache = simple_cache.encode(\n",
    "    repeated_sentences, batch_size=32)\n",
    "simple_cache_time = time.time() - start_time\n",
    "print(f\"Time with simple cache: {simple_cache_time:.4f}s\")\n",
    "print(f\"Simple cache stats: {simple_cache.get_stats()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-World Applications\n",
    "Let's discuss how batching and caching apply in real-world scenarios:\n",
    "\n",
    "- Search applications: When users search for similar content, you can cache embeddings for frequently searched terms.\n",
    "- Content recommendation systems: Product descriptions, articles, or other items that appear frequently can have their embeddings cached.\n",
    "- Customer support systems: Common questions or support tickets can benefit from cached embeddings.\n",
    "- Document processing pipelines: When processing documents, batch them together for maximum throughput.\n",
    "- API services: Implement caching layers to avoid regenerating embeddings for common requests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Production Best Practices\n",
    "Here are some practical tips for implementing these techniques in production:\n",
    "\n",
    "- Choose batch size based on your latency requirements: If you need real-time responses, smaller batches may be better.\n",
    "- Monitor your cache hit rate: A low hit rate might mean your cache strategy needs adjustment.\n",
    "- Consider cache expiration policies: In some applications, content changes over time and cached embeddings may need refreshing.\n",
    "- Use persistent caching for production: Consider Redis, Memcached, or database-backed caches for production systems.\n",
    "- Scale horizontally: For very high throughput needs, consider distributing embedding generation across multiple servers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
