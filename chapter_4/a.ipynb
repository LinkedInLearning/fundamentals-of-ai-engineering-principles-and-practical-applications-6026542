{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Understanding Text Embeddings: From Words to Vectors\n",
        "\n",
        "This notebook explores text embeddings, a fundamental concept in natural language processing. We'll investigate how computers understand semantic meaning by converting words and sentences into numerical vectors.\n",
        "\n",
        "**What Are Embeddings?**\n",
        "- Dense numerical representations of data in a continuous vector space\n",
        "- Similar meanings are positioned close together \n",
        "- Relative positions capture semantic relationships"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2mUsing Python 3.12.9 environment at: /workspaces/fundamentals-of-ai-engineering-principles-and-practical-applications-6026542/.venv\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m44 packages\u001b[0m \u001b[2min 226ms\u001b[0m\u001b[0m                                        \u001b[0m\n",
            "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 6ms\u001b[0m\u001b[0m                                                \n",
            "\u001b[2K░░░░░░░░░░░░░░░░░░░░ [0/1] \u001b[2mInstalling wheels...                                 \u001b[0m\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mFailed to hardlink files; falling back to full copy. This may lead to degraded performance.\n",
            "         If the cache and target directories are on different filesystems, hardlinking may not be supported.\n",
            "         If this is intentional, set `export UV_LINK_MODE=copy` or use `--link-mode=copy` to suppress this warning.\u001b[0m\n",
            "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 136ms\u001b[0m\u001b[0m==4.0.2                         \u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1msentence-transformers\u001b[0m\u001b[2m==4.0.2\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Install the required packages\n",
        "!uv pip install -U sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Set up matplotlib\n",
        "try:\n",
        "    plt.style.use('seaborn-v0_8-whitegrid')\n",
        "except:\n",
        "    try:\n",
        "        plt.style.use('seaborn-whitegrid')  # Fallback for older versions\n",
        "    except:\n",
        "        pass  # Default style if neither is available\n",
        "        \n",
        "plt.rcParams['figure.figsize'] = (10, 7)\n",
        "np.random.seed(42)  # For reproducibility"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Loading an Embedding Model\n",
        "\n",
        "We'll use the `all-MiniLM-L6-v2` model, which creates 384-dimensional embeddings and is optimized for semantic similarity tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'init_empty_weights' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load the pre-trained model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m model = \u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msentence-transformers/all-MiniLM-L6-v2\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel: all-MiniLM-L6-v2\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEmbedding dimension: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel.get_sentence_embedding_dimension()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/fundamentals-of-ai-engineering-principles-and-practical-applications-6026542/.venv/lib/python3.12/site-packages/sentence_transformers/SentenceTransformer.py:309\u001b[39m, in \u001b[36mSentenceTransformer.__init__\u001b[39m\u001b[34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data, backend)\u001b[39m\n\u001b[32m    300\u001b[39m         model_name_or_path = __MODEL_HUB_ORGANIZATION__ + \u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m + model_name_or_path\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentence_transformer_model(\n\u001b[32m    303\u001b[39m     model_name_or_path,\n\u001b[32m    304\u001b[39m     token,\n\u001b[32m   (...)\u001b[39m\u001b[32m    307\u001b[39m     local_files_only=local_files_only,\n\u001b[32m    308\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m309\u001b[39m     modules, \u001b[38;5;28mself\u001b[39m.module_kwargs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_sbert_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    310\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    321\u001b[39m     modules = \u001b[38;5;28mself\u001b[39m._load_auto_model(\n\u001b[32m    322\u001b[39m         model_name_or_path,\n\u001b[32m    323\u001b[39m         token=token,\n\u001b[32m   (...)\u001b[39m\u001b[32m    330\u001b[39m         config_kwargs=config_kwargs,\n\u001b[32m    331\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/fundamentals-of-ai-engineering-principles-and-practical-applications-6026542/.venv/lib/python3.12/site-packages/sentence_transformers/SentenceTransformer.py:1802\u001b[39m, in \u001b[36mSentenceTransformer._load_sbert_model\u001b[39m\u001b[34m(self, model_name_or_path, token, cache_folder, revision, trust_remote_code, local_files_only, model_kwargs, tokenizer_kwargs, config_kwargs)\u001b[39m\n\u001b[32m   1799\u001b[39m \u001b[38;5;66;03m# Try to initialize the module with a lot of kwargs, but only if the module supports them\u001b[39;00m\n\u001b[32m   1800\u001b[39m \u001b[38;5;66;03m# Otherwise we fall back to the load method\u001b[39;00m\n\u001b[32m   1801\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1802\u001b[39m     module = \u001b[43mmodule_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1803\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   1804\u001b[39m     module = module_class.load(model_name_or_path)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/fundamentals-of-ai-engineering-principles-and-practical-applications-6026542/.venv/lib/python3.12/site-packages/sentence_transformers/models/Transformer.py:81\u001b[39m, in \u001b[36mTransformer.__init__\u001b[39m\u001b[34m(self, model_name_or_path, max_seq_length, model_args, tokenizer_args, config_args, cache_dir, do_lower_case, tokenizer_name_or_path, backend)\u001b[39m\n\u001b[32m     78\u001b[39m     config_args = {}\n\u001b[32m     80\u001b[39m config, is_peft_model = \u001b[38;5;28mself\u001b[39m._load_config(model_name_or_path, cache_dir, backend, config_args)\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_peft_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m max_seq_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmodel_max_length\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m tokenizer_args:\n\u001b[32m     84\u001b[39m     tokenizer_args[\u001b[33m\"\u001b[39m\u001b[33mmodel_max_length\u001b[39m\u001b[33m\"\u001b[39m] = max_seq_length\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/fundamentals-of-ai-engineering-principles-and-practical-applications-6026542/.venv/lib/python3.12/site-packages/sentence_transformers/models/Transformer.py:181\u001b[39m, in \u001b[36mTransformer._load_model\u001b[39m\u001b[34m(self, model_name_or_path, config, cache_dir, backend, is_peft_model, **model_args)\u001b[39m\n\u001b[32m    179\u001b[39m     \u001b[38;5;28mself\u001b[39m._load_mt5_model(model_name_or_path, config, cache_dir, **model_args)\n\u001b[32m    180\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m181\u001b[39m     \u001b[38;5;28mself\u001b[39m.auto_model = \u001b[43mAutoModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_peft_model:\n\u001b[32m    186\u001b[39m     \u001b[38;5;28mself\u001b[39m._load_peft_model(model_name_or_path, config, cache_dir, **model_args, **adapter_only_kwargs)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/fundamentals-of-ai-engineering-principles-and-practical-applications-6026542/.venv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:571\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    569\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    570\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m571\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    572\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    573\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    574\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    575\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    576\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    577\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/fundamentals-of-ai-engineering-principles-and-practical-applications-6026542/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:279\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    277\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    281\u001b[39m     torch.set_default_dtype(old_dtype)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/fundamentals-of-ai-engineering-principles-and-practical-applications-6026542/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:4333\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4330\u001b[39m config.name_or_path = pretrained_model_name_or_path\n\u001b[32m   4332\u001b[39m \u001b[38;5;66;03m# Instantiate model.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4333\u001b[39m model_init_context = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_init_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mis_quantized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_is_ds_init_called\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4335\u001b[39m config = copy.deepcopy(config)  \u001b[38;5;66;03m# We do not want to modify the config inplace in from_pretrained.\u001b[39;00m\n\u001b[32m   4336\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(config, \u001b[33m\"\u001b[39m\u001b[33m_attn_implementation_autoset\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/fundamentals-of-ai-engineering-principles-and-practical-applications-6026542/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:3736\u001b[39m, in \u001b[36mPreTrainedModel.get_init_context\u001b[39m\u001b[34m(cls, is_quantized, _is_ds_init_called)\u001b[39m\n\u001b[32m   3734\u001b[39m         init_contexts.append(set_quantized_state())\n\u001b[32m   3735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3736\u001b[39m     init_contexts = [no_init_weights(), \u001b[43minit_empty_weights\u001b[49m()]\n\u001b[32m   3738\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m init_contexts\n",
            "\u001b[31mNameError\u001b[39m: name 'init_empty_weights' is not defined"
          ]
        }
      ],
      "source": [
        "# Load the pre-trained model\n",
        "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "print(f\"Model: all-MiniLM-L6-v2\")\n",
        "print(f\"Embedding dimension: {model.get_sentence_embedding_dimension()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Creating and Examining Embeddings\n",
        "\n",
        "Let's create embeddings for some example sentences grouped by topic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence 1 (AI/ML): I love machine learning and artificial intelligence.\n",
            "Sentence 2 (AI/ML): AI and ML are fascinating fields of study.\n",
            "Sentence 3 (Weather): The weather is beautiful today.\n",
            "Sentence 4 (Weather): It's a sunny day with clear skies.\n",
            "Sentence 5 (Python): Python is my favorite programming language.\n",
            "Sentence 6 (Python): I enjoy coding in Python for data analysis.\n"
          ]
        }
      ],
      "source": [
        "# Example sentences grouped by topic\n",
        "sentences = [\n",
        "    # AI/ML related sentences\n",
        "    \"I love machine learning and artificial intelligence.\",\n",
        "    \"AI and ML are fascinating fields of study.\",\n",
        "    \n",
        "    # Weather related sentences\n",
        "    \"The weather is beautiful today.\",\n",
        "    \"It's a sunny day with clear skies.\",\n",
        "    \n",
        "    # Python related sentences\n",
        "    \"Python is my favorite programming language.\",\n",
        "    \"I enjoy coding in Python for data analysis.\"\n",
        "]\n",
        "\n",
        "# Topic labels for visualization\n",
        "topics = ['AI/ML', 'AI/ML', 'Weather', 'Weather', 'Python', 'Python']\n",
        "\n",
        "# Display our sentences with their topics\n",
        "for i, (sentence, topic) in enumerate(zip(sentences, topics)):\n",
        "    print(f\"Sentence {i+1} ({topic}): {sentence}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Create embeddings for our sentences\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m embeddings = \u001b[43mmodel\u001b[49m.encode(sentences)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Display embedding information\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mShape of each embedding: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00membeddings[\u001b[32m0\u001b[39m].shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "# Create embeddings for our sentences\n",
        "embeddings = model.encode(sentences)\n",
        "\n",
        "# Display embedding information\n",
        "print(f\"Shape of each embedding: {embeddings[0].shape}\")\n",
        "print(f\"Number of embeddings: {len(embeddings)}\")\n",
        "\n",
        "# Show a snippet of the first embedding\n",
        "print(f\"\\nFirst 10 dimensions of first embedding: {embeddings[0][:10]}\")\n",
        "print(f\"Min: {embeddings[0].min():.4f}, Max: {embeddings[0].max():.4f}, Mean: {embeddings[0].mean():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Measuring Similarity with Cosine Similarity\n",
        "\n",
        "**Cosine Similarity Explained:**\n",
        "- Measures the cosine of the angle between two vectors\n",
        "- Ranges from -1 (opposite) to 1 (identical)\n",
        "- Higher values indicate greater semantic similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'embeddings' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Calculate cosine similarity between all pairs of embeddings\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m similarity_matrix = cosine_similarity(\u001b[43membeddings\u001b[49m)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Display the similarity matrix\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCosine Similarity Matrix:\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mNameError\u001b[39m: name 'embeddings' is not defined"
          ]
        }
      ],
      "source": [
        "# Calculate cosine similarity between all pairs of embeddings\n",
        "similarity_matrix = cosine_similarity(embeddings)\n",
        "\n",
        "# Display the similarity matrix\n",
        "print(\"Cosine Similarity Matrix:\")\n",
        "np.set_printoptions(precision=4, suppress=True)\n",
        "print(similarity_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create labels for our heatmap\n",
        "labels = [f\"S{i+1}: {topic}\" for i, topic in enumerate(topics)]\n",
        "\n",
        "# Create a heatmap of the similarity matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(similarity_matrix, annot=True, cmap='viridis', xticklabels=labels, yticklabels=labels)\n",
        "plt.title('Cosine Similarity Heatmap')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Heatmap Interpretation:\")\n",
        "print(\"- The diagonal (1.0 values) shows each sentence's similarity with itself\")\n",
        "print(\"- Brighter blocks show high similarity between sentences on the same topic\")\n",
        "print(\"- Darker areas show lower similarity between sentences on different topics\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Visualizing Embeddings in 2D Space\n",
        "\n",
        "We'll use PCA to reduce our 384-dimensional embeddings to 2D for visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reduce embeddings to 2 dimensions using PCA\n",
        "pca = PCA(n_components=2)\n",
        "embeddings_2d = pca.fit_transform(embeddings)\n",
        "\n",
        "# Set up colors for topics\n",
        "topic_colors = {'AI/ML': 'red', 'Weather': 'blue', 'Python': 'green'}\n",
        "colors = [topic_colors[topic] for topic in topics]\n",
        "\n",
        "# Plot the 2D embeddings\n",
        "plt.figure(figsize=(12, 8))\n",
        "for i, (x, y) in enumerate(embeddings_2d):\n",
        "    plt.scatter(x, y, c=colors[i], s=100, alpha=0.7, edgecolors='black')\n",
        "    plt.annotate(f\"S{i+1}\", \n",
        "                xy=(x, y), \n",
        "                xytext=(5, 5), \n",
        "                textcoords='offset points',\n",
        "                fontsize=12,\n",
        "                weight='bold')\n",
        "\n",
        "# Add a legend\n",
        "for topic, color in topic_colors.items():\n",
        "    plt.scatter([], [], c=color, label=topic, s=100, alpha=0.7)\n",
        "plt.legend(loc='upper right')\n",
        "\n",
        "# Add title and labels\n",
        "plt.title('2D PCA Projection of Sentence Embeddings', fontsize=15)\n",
        "plt.xlabel(f'Principal Component 1 (Explained Variance: {pca.explained_variance_ratio_[0]:.2%})', fontsize=12)\n",
        "plt.ylabel(f'Principal Component 2 (Explained Variance: {pca.explained_variance_ratio_[1]:.2%})', fontsize=12)\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Notice how sentences on the same topic cluster together in the 2D space.\")\n",
        "print(f\"The two principal components capture {sum(pca.explained_variance_ratio_):.2%} of the total variance.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Testing with New Sentences\n",
        "\n",
        "Let's see how our model handles new sentences related to our original topics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define new sentences\n",
        "new_sentences = [\n",
        "    \"Deep learning has revolutionized computer vision.\",  # AI/ML related\n",
        "    \"The forecast predicts rain for tomorrow.\",           # Weather related\n",
        "    \"NumPy and Pandas are essential Python libraries.\"    # Python related\n",
        "]\n",
        "\n",
        "# Create embeddings for the new sentences\n",
        "new_embeddings = model.encode(new_sentences)\n",
        "\n",
        "# Calculate similarity between new and original sentences\n",
        "similarity_to_original = cosine_similarity(new_embeddings, embeddings)\n",
        "\n",
        "# Find the most similar original sentence for each new sentence\n",
        "for i, new_sent in enumerate(new_sentences):\n",
        "    # Get index of most similar original sentence\n",
        "    most_similar_idx = np.argmax(similarity_to_original[i])\n",
        "    print(f\"\\nNew: \\\"{new_sent}\\\"\")\n",
        "    print(f\"Most similar to: \\\"{sentences[most_similar_idx]}\\\"\")\n",
        "    print(f\"Similarity score: {similarity_to_original[i][most_similar_idx]:.4f}\")\n",
        "    print(f\"Topic: {topics[most_similar_idx]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Visualizing Original and New Sentences Together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Combine original and new embeddings\n",
        "all_embeddings = np.vstack([embeddings, new_embeddings])\n",
        "all_topics = topics + ['AI/ML', 'Weather', 'Python']\n",
        "\n",
        "# Project to 2D using PCA\n",
        "pca = PCA(n_components=2)\n",
        "all_embeddings_2d = pca.fit_transform(all_embeddings)\n",
        "\n",
        "# Create visualization\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Plot original sentences\n",
        "for i in range(len(sentences)):\n",
        "    x, y = all_embeddings_2d[i]\n",
        "    plt.scatter(x, y, c=topic_colors[all_topics[i]], s=100, alpha=0.7, edgecolors='black')\n",
        "    plt.annotate(f\"S{i+1}\", xy=(x, y), xytext=(5, 5), textcoords='offset points', fontsize=10)\n",
        "\n",
        "# Plot new sentences with star markers\n",
        "for i in range(len(sentences), len(sentences) + len(new_sentences)):\n",
        "    x, y = all_embeddings_2d[i]\n",
        "    plt.scatter(x, y, c=topic_colors[all_topics[i]], s=150, alpha=0.9, marker='*', edgecolors='black')\n",
        "    plt.annotate(f\"N{i-len(sentences)+1}\", xy=(x, y), xytext=(5, 5), textcoords='offset points', fontsize=10, weight='bold')\n",
        "\n",
        "# Add a legend\n",
        "for topic, color in topic_colors.items():\n",
        "    plt.scatter([], [], c=color, label=topic, s=100, alpha=0.7)\n",
        "plt.scatter([], [], c='gray', marker='o', s=100, label='Original', alpha=0.7)\n",
        "plt.scatter([], [], c='gray', marker='*', s=150, label='New', alpha=0.9)\n",
        "plt.legend(loc='upper right')\n",
        "\n",
        "plt.title('PCA Projection of Original and New Sentences', fontsize=15)\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Notice how new sentences (stars) appear close to their semantically related original sentences.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Real-World Applications of Embeddings\n",
        "\n",
        "Embeddings power many modern AI applications including:\n",
        "\n",
        "1. **Semantic Search**: Finding documents based on meaning rather than just keywords\n",
        "2. **Document Clustering**: Automatically grouping similar documents\n",
        "3. **Recommendation Systems**: Suggesting similar items based on semantic content\n",
        "4. **Question Answering**: Finding relevant information to answer queries\n",
        "5. **Retrieval Augmented Generation (RAG)**: Combining LLMs with knowledge bases using embeddings\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "In this notebook, we've explored the fundamentals of text embeddings:\n",
        "- How embeddings represent text as numerical vectors capturing semantic meaning\n",
        "- Using cosine similarity to measure semantic relationships\n",
        "- Visualizing high-dimensional embeddings in 2D space\n",
        "- Testing embedding models with new sentences\n",
        "\n",
        "Embeddings serve as the bridge between human language and machine understanding, forming the foundation of many modern NLP systems."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
