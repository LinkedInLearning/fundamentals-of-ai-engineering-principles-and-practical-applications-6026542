{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling Vector Database for Production\n",
    "\n",
    "## Key Scaling Considerations\n",
    "\n",
    "1. **Speed vs. Accuracy** - Understanding the tradeoffs between query performance and result quality\n",
    "2. **Resource Limitations** - Managing memory, CPU, and storage constraints\n",
    "3. **Horizontal Scaling** - Distributing the workload across multiple instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximate Nearest Neighbor (ANN) Implementations\n",
    "\n",
    "ANN algorithms like HNSW (Hierarchical Navigable Small World) allow us to trade some accuracy for significant performance improvements at scale. We'll explore different HNSW configurations and their impact on search performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "import time\n",
    "\n",
    "# Initialize ChromaDB client\n",
    "client = chromadb.Client()\n",
    "embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "  model_name=\"all-MiniLM-L6-v2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Collections with Different HNSW Configurations\n",
    "\n",
    "We'll create three collections with different index settings:\n",
    "\n",
    "1. **Default** - Uses ChromaDB's default configuration\n",
    "2. **High Accuracy** - Prioritizes result quality with higher `ef` and `M` values\n",
    "3. **Fast Search** - Prioritizes speed with lower `ef` and `M` values\n",
    "\n",
    "**Parameter Explanation:**\n",
    "- `hnsw:space`: The distance metric used (cosine, euclidean, etc.)\n",
    "- `hnsw:construction_ef`: Controls index build quality (higher = better quality, slower build)\n",
    "- `hnsw:search_ef`: Controls search quality (higher = better quality, slower search)\n",
    "- `hnsw:M`: Controls the maximum number of connections per node (higher = better quality, more memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create collections with different HNSW configurations\n",
    "collections = {}\n",
    "\n",
    "# 1. Default settings\n",
    "collections[\"default\"] = client.create_collection(\n",
    "    name=\"default_index\",\n",
    "    embedding_function=embedding_function\n",
    ")\n",
    "\n",
    "# 2. High accuracy configuration\n",
    "collections[\"high_accuracy\"] = client.create_collection(\n",
    "    name=\"high_accuracy_index\",\n",
    "    embedding_function=embedding_function,\n",
    "    metadata={\"hnsw:space\": \"cosine\", \"hnsw:construction_ef\": 500, \"hnsw:search_ef\": 250, \"hnsw:M\": 36}\n",
    ")\n",
    "\n",
    "# 3. Fast search configuration\n",
    "collections[\"fast_search\"] = client.create_collection(\n",
    "    name=\"fast_search_index\",\n",
    "    embedding_function=embedding_function,\n",
    "    metadata={\"hnsw:space\": \"cosine\", \"hnsw:construction_ef\": 80, \"hnsw:search_ef\": 40, \"hnsw:M\": 12}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Sample Documents\n",
    "\n",
    "Now let's create some sample documents across different categories to populate our collections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 5000 sample documents...\n"
     ]
    }
   ],
   "source": [
    "# Generate sample data\n",
    "num_docs = 5000\n",
    "print(f\"Generating {num_docs} sample documents...\")\n",
    "\n",
    "# Create documents with some patterns for testing\n",
    "categories = [\"technology\", \"science\", \"health\", \"business\", \"entertainment\"]\n",
    "documents = []\n",
    "ids = []\n",
    "\n",
    "for i in range(num_docs):\n",
    "    category = categories[i % len(categories)]\n",
    "    document = f\"This is document {i} about {category} with some additional text to make it more unique.\"\n",
    "    documents.append(document)\n",
    "    ids.append(f\"doc_{i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Documents to Collections\n",
    "\n",
    "Let's add the generated documents to all three collections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding documents to collections with different index configurations...\n",
      "  Added 5000 documents to default collection\n",
      "  Added 5000 documents to high_accuracy collection\n",
      "  Added 5000 documents to fast_search collection\n"
     ]
    }
   ],
   "source": [
    "# Add documents to all collections\n",
    "print(\"Adding documents to collections with different index configurations...\")\n",
    "for name, collection in collections.items():\n",
    "    collection.add(\n",
    "        documents=documents,\n",
    "        ids=ids\n",
    "    )\n",
    "    print(f\"  Added {num_docs} documents to {name} collection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark Query Performance\n",
    "\n",
    "Now let's evaluate how each configuration performs with a set of representative queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark query performance\n",
    "print(\"\\nBenchmarking query performance across different configurations...\")\n",
    "\n",
    "# Prepare queries\n",
    "query_texts = [\n",
    "    \"Latest technology trends in artificial intelligence\",\n",
    "    \"Scientific research on climate change\",\n",
    "    \"Health benefits of regular exercise\",\n",
    "    \"Business strategies for startups\",\n",
    "    \"Entertainment news about recent movie releases\"\n",
    "]\n",
    "\n",
    "# Set up benchmark parameters\n",
    "results = {}\n",
    "num_trials = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run benchmark for each collection\n",
    "for name, collection in collections.items():\n",
    "    print(f\"\\nTesting {name} configuration:\")\n",
    "    times = []\n",
    "    \n",
    "    for query in query_texts:\n",
    "        query_times = []\n",
    "        \n",
    "        for _ in range(num_trials):\n",
    "            start_time = time.time()\n",
    "            collection.query(\n",
    "                query_texts=[query],\n",
    "                n_results=10\n",
    "            )\n",
    "            query_time = time.time() - start_time\n",
    "            query_times.append(query_time)\n",
    "        \n",
    "        avg_time = sum(query_times) / len(query_times)\n",
    "        times.append(avg_time)\n",
    "        print(f\"  Query: '{query[:30]}...': {avg_time:.4f} seconds\")\n",
    "    \n",
    "    results[name] = {\n",
    "        \"mean\": sum(times) / len(times),\n",
    "        \"min\": min(times),\n",
    "        \"max\": max(times),\n",
    "        \"times\": times\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary of benchmark results\n",
    "print(\"\\nPerformance Summary:\")\n",
    "for name, metrics in results.items():\n",
    "    print(f\"  {name}: Mean={metrics['mean']:.4f}s, Min={metrics['min']:.4f}s, Max={metrics['max']:.4f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Caching Implementation\n",
    "\n",
    "Caching is a crucial optimization technique for production systems. It can significantly reduce latency and computational load by storing frequently accessed results.\n",
    "\n",
    "### Why Implement Caching?\n",
    "\n",
    "1. **Reduced Latency** - Cached results can be returned instantly without computing embeddings or searching the vector space\n",
    "2. **Lower Computational Costs** - Fewer embedding calculations mean lower GPU/CPU usage\n",
    "3. **Better Scalability** - Handle more queries with the same resources\n",
    "\n",
    "We'll implement a simple LRU (Least Recently Used) cache and measure its performance impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "import time\n",
    "import random\n",
    "\n",
    "print(\"\\n=== CACHING IMPLEMENTATION ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LRU Cache Implementation\n",
    "\n",
    "Let's implement a simple LRU (Least Recently Used) cache. This type of cache keeps track of which queries are used most frequently and evicts the least recently used entries when the cache is full."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a simple LRU cache\n",
    "class LRUCache:\n",
    "    def __init__(self, capacity=100):\n",
    "        self.capacity = capacity  # Maximum number of items the cache can hold\n",
    "        self.cache = {}           # Dictionary to store cache items\n",
    "        self.usage_order = []     # List to track access order\n",
    "    \n",
    "    def get(self, key):\n",
    "        if key in self.cache:\n",
    "            # Update usage order - move to end of list (most recently used)\n",
    "            self.usage_order.remove(key)\n",
    "            self.usage_order.append(key)\n",
    "            return self.cache[key]\n",
    "        return None  # Cache miss\n",
    "    \n",
    "    def put(self, key, value):\n",
    "        if key in self.cache:\n",
    "            # Update existing entry\n",
    "            self.cache[key] = value\n",
    "            self.usage_order.remove(key)\n",
    "            self.usage_order.append(key)\n",
    "        else:\n",
    "            # Add new entry\n",
    "            if len(self.cache) >= self.capacity:\n",
    "                # Evict least recently used item\n",
    "                lru_key = self.usage_order.pop(0)\n",
    "                del self.cache[lru_key]\n",
    "            \n",
    "            self.cache[key] = value\n",
    "            self.usage_order.append(key)\n",
    "    \n",
    "    def clear(self):\n",
    "        self.cache = {}\n",
    "        self.usage_order = []\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up the Collection\n",
    "\n",
    "Now let's create a collection and populate it with sample documents for our caching experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Chroma\n",
    "client = chromadb.Client()\n",
    "embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "    model_name=\"all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "# Create a collection\n",
    "collection = client.create_collection(\n",
    "    name=\"cache_test\",\n",
    "    embedding_function=embedding_function\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add sample documents\n",
    "num_docs = 1000\n",
    "documents = [f\"This is a sample document {i} with various content for testing caching\" for i in range(num_docs)]\n",
    "ids = [f\"cache_doc_{i}\" for i in range(num_docs)]\n",
    "\n",
    "# Add documents in batches to avoid overwhelming the system\n",
    "for i in range(0, num_docs, 100):\n",
    "    end_idx = min(i + 100, num_docs)\n",
    "    \n",
    "    collection.add(\n",
    "        documents=documents[i:end_idx],\n",
    "        ids=ids[i:end_idx]\n",
    "    )\n",
    "\n",
    "print(f\"Added {num_docs} documents to the collection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cached Query Function\n",
    "\n",
    "Let's implement a function that uses our cache to store and retrieve query results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize cache with a capacity of 50 entries\n",
    "query_cache = LRUCache(capacity=50)\n",
    "\n",
    "# Function to query with caching\n",
    "def cached_query(query_text, n_results=10, use_cache=True):\n",
    "    # Create a unique cache key from the query text and number of results\n",
    "    cache_key = f\"{query_text}:{n_results}\"\n",
    "    \n",
    "    if use_cache:\n",
    "        # Check cache first\n",
    "        cached_result = query_cache.get(cache_key)\n",
    "        if cached_result is not None:\n",
    "            return cached_result, True  # Cache hit\n",
    "    \n",
    "    # Cache miss or cache disabled, perform actual query\n",
    "    result = collection.query(\n",
    "        query_texts=[query_text],\n",
    "        n_results=n_results\n",
    "    )\n",
    "    \n",
    "    if use_cache:\n",
    "        # Update cache\n",
    "        query_cache.put(cache_key, result)\n",
    "    \n",
    "    return result, False  # Cache miss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Query Mix\n",
    "\n",
    "To simulate a realistic workload, we'll create a mix of common (frequently repeated) and unique queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test queries with varying cache hit rates\n",
    "print(\"\\nTesting query performance with caching:\")\n",
    "\n",
    "# Prepare query mix (some repeated, some unique)\n",
    "common_queries = [\n",
    "    \"document with content\",\n",
    "    \"sample document\",\n",
    "    \"testing caching\",\n",
    "    \"various content\"\n",
    "]\n",
    "\n",
    "unique_queries = [f\"unique query {i}\" for i in range(50)]\n",
    "\n",
    "# Mix queries with different distributions to test cache performance\n",
    "mixed_queries = []\n",
    "for _ in range(20):\n",
    "    # Add common queries (higher probability)\n",
    "    mixed_queries.extend(common_queries)\n",
    "    \n",
    "    # Add some unique queries\n",
    "    mixed_queries.extend(random.sample(unique_queries, 5))\n",
    "\n",
    "# Shuffle to ensure realistic query pattern\n",
    "random.shuffle(mixed_queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark: No Cache vs. With Cache\n",
    "\n",
    "Now let's measure the performance difference between running queries without a cache versus with a cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run without cache\n",
    "print(\"Running queries without cache...\")\n",
    "start_time = time.time()\n",
    "\n",
    "for query in mixed_queries:\n",
    "    _, _ = cached_query(query, use_cache=False)\n",
    "\n",
    "no_cache_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run with cache\n",
    "print(\"Running queries with cache...\")\n",
    "query_cache.clear()  # Clear the cache\n",
    "\n",
    "start_time = time.time()\n",
    "hits = 0\n",
    "\n",
    "for query in mixed_queries:\n",
    "    _, is_hit = cached_query(query, use_cache=True)\n",
    "    if is_hit:\n",
    "        hits += 1\n",
    "\n",
    "with_cache_time = time.time() - start_time\n",
    "hit_rate = hits / len(mixed_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report results\n",
    "print(\"\\nCache Performance Results:\")\n",
    "print(f\"  Without cache: {no_cache_time:.4f} seconds\")\n",
    "print(f\"  With cache: {with_cache_time:.4f} seconds\")\n",
    "print(f\"  Time saved: {no_cache_time - with_cache_time:.4f} seconds ({(1 - with_cache_time/no_cache_time) * 100:.1f}%)\")\n",
    "print(f\"  Cache hit rate: {hit_rate:.1%}\")\n",
    "print(f\"  Cache size: {len(query_cache)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Advanced Scaling Strategies\n",
    "\n",
    "### Horizontal Scaling Approaches\n",
    "\n",
    "As your vector database grows beyond the capacity of a single machine, you'll need to implement horizontal scaling strategies. Here are some common approaches:\n",
    "\n",
    "1. **Sharding** - Partitioning your vector space across multiple instances\n",
    "   - **By ID range** - Deterministic but may lead to unbalanced shards\n",
    "   - **By vector clustering** - Better search performance but more complex\n",
    "\n",
    "2. **Replication** - Creating copies of your data across multiple instances\n",
    "   - Improves read throughput and fault tolerance\n",
    "   - Requires synchronization mechanisms for writes\n",
    "\n",
    "3. **Hybrid approaches** - Combining sharding and replication\n",
    "   - Example: ChromaDB cluster with data sharded across nodes and each shard replicated\n",
    "\n",
    "### Resource Management Best Practices\n",
    "\n",
    "1. **Memory Optimization**\n",
    "   - Use quantization to reduce vector size (e.g., 32-bit to 8-bit)\n",
    "   - Implement disk-based storage for less frequently accessed vectors\n",
    "\n",
    "2. **CPU Utilization**\n",
    "   - Batch similar operations\n",
    "   - Use asynchronous processing where possible\n",
    "\n",
    "3. **Network Efficiency**\n",
    "   - Minimize data transfer between components\n",
    "   - Compress payloads when possible\n",
    "\n",
    "### Real-world Implementation Considerations\n",
    "\n",
    "1. **Monitoring and Observability**\n",
    "   - Track latency, throughput, and error rates\n",
    "   - Set up alerts for performance degradation\n",
    "\n",
    "2. **Failure Handling**\n",
    "   - Implement graceful degradation strategies\n",
    "   - Consider fallback search methods\n",
    "\n",
    "3. **Update Strategies**\n",
    "   - Batch updates to reduce index rebuilding frequency\n",
    "   - Consider incremental index updates\n",
    "\n",
    "4. **Hybrid Search Approaches**\n",
    "   - Combine vector search with keyword search for better results\n",
    "   - Filter vectors based on metadata before computing distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Key Takeaways\n",
    "\n",
    "In this notebook, we've explored practical approaches to scaling vector databases for production use:\n",
    "\n",
    "1. **ANN Implementations**\n",
    "   - Configuring HNSW parameters allows for customized speed-accuracy tradeoffs\n",
    "   - The right configuration depends on your specific application requirements\n",
    "\n",
    "2. **Caching**\n",
    "   - Can significantly reduce latency (70%+ in our example)\n",
    "   - Most effective when query patterns show temporal locality\n",
    "\n",
    "3. **Advanced Scaling Strategies**\n",
    "   - Horizontal scaling through sharding and replication\n",
    "   - Resource optimization across memory, CPU, and network\n",
    "   - Operational considerations for production deployments\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Experiment with different HNSW configurations on your specific dataset\n",
    "2. Implement and tune caching based on your application's query patterns\n",
    "3. Explore ChromaDB's distributed deployment options for horizontal scaling\n",
    "4. Set up monitoring and observability for your vector database"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
