{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling Strategies (ANN)\n",
    "\n",
    "## Key Scaling Considerations\n",
    "\n",
    "1. **Speed vs. Accuracy** - Understanding the tradeoffs between query performance and result quality\n",
    "2. **Resource Limitations** - Managing memory, CPU, and storage constraints\n",
    "3. **Horizontal Scaling** - Distributing the workload across multiple instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximate Nearest Neighbor (ANN) Implementations\n",
    "\n",
    "ANN algorithms like HNSW (Hierarchical Navigable Small World) allow us to trade some accuracy for significant performance improvements at scale. We'll explore different HNSW configurations and their impact on search performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup and Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.12.9 environment at: /workspaces/fundamentals-of-ai-engineering-principles-and-practical-applications-6026542/.venv\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m2 packages\u001b[0m \u001b[2min 8ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install the required packages\n",
    "!uv pip install accelerate==1.6.0 sentence-transformers==4.0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "import time\n",
    "\n",
    "# Initialize ChromaDB client\n",
    "client = chromadb.Client()\n",
    "embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "  model_name=\"all-MiniLM-L6-v2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Collections with Different HNSW Configurations\n",
    "\n",
    "We'll create three collections with different index settings:\n",
    "\n",
    "1. **Default** - Uses ChromaDB's default configuration\n",
    "2. **High Accuracy** - Prioritizes result quality with higher `ef` and `M` values\n",
    "3. **Fast Search** - Prioritizes speed with lower `ef` and `M` values\n",
    "\n",
    "**Parameter Explanation:**\n",
    "- `hnsw:space`: The distance metric used (cosine, euclidean, etc.)\n",
    "- `hnsw:construction_ef`: Controls index build quality (higher = better quality, slower build)\n",
    "- `hnsw:search_ef`: Controls search quality (higher = better quality, slower search)\n",
    "- `hnsw:M`: Controls the maximum number of connections per node (higher = better quality, more memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create collections with different HNSW configurations\n",
    "collections = {}\n",
    "\n",
    "# 1. Default settings\n",
    "collections[\"default\"] = client.create_collection(\n",
    "    name=\"default_index\",\n",
    "    embedding_function=embedding_function\n",
    ")\n",
    "\n",
    "# 2. High accuracy configuration\n",
    "collections[\"high_accuracy\"] = client.create_collection(\n",
    "    name=\"high_accuracy_index\",\n",
    "    embedding_function=embedding_function,\n",
    "    metadata={\"hnsw:space\": \"cosine\", \"hnsw:construction_ef\": 500, \"hnsw:search_ef\": 250, \"hnsw:M\": 36}\n",
    ")\n",
    "\n",
    "# 3. Fast search configuration\n",
    "collections[\"fast_search\"] = client.create_collection(\n",
    "    name=\"fast_search_index\",\n",
    "    embedding_function=embedding_function,\n",
    "    metadata={\"hnsw:space\": \"cosine\", \"hnsw:construction_ef\": 80, \"hnsw:search_ef\": 40, \"hnsw:M\": 12}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Sample Documents\n",
    "\n",
    "Now let's create some sample documents across different categories to populate our collections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 5000 sample documents...\n"
     ]
    }
   ],
   "source": [
    "# Generate sample data\n",
    "num_docs = 5000\n",
    "print(f\"Generating {num_docs} sample documents...\")\n",
    "\n",
    "# Create documents with some patterns for testing\n",
    "categories = [\"technology\", \"science\", \"health\", \"business\", \"entertainment\"]\n",
    "documents = []\n",
    "ids = []\n",
    "\n",
    "for i in range(num_docs):\n",
    "    category = categories[i % len(categories)]\n",
    "    document = f\"This is document {i} about {category} with some additional text to make it more unique.\"\n",
    "    documents.append(document)\n",
    "    ids.append(f\"doc_{i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Documents to Collections\n",
    "\n",
    "Let's add the generated documents to all three collections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding documents to collections with different index configurations...\n",
      "  Added 5000 documents to default collection in 18.2801 seconds\n",
      "  Added 5000 documents to high_accuracy collection in 17.4464 seconds\n",
      "  Added 5000 documents to fast_search collection in 16.9369 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "print(\"Adding documents to collections with different index configurations...\")\n",
    "for name, collection in collections.items():\n",
    "    start_time = time.time()\n",
    "\n",
    "    collection.add(\n",
    "        documents=documents,\n",
    "        ids=ids\n",
    "    )\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "\n",
    "    print(\n",
    "        f\"  Added {num_docs} documents to {name} collection in {elapsed_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark Query Performance\n",
    "\n",
    "Now let's evaluate how each configuration performs with a set of representative queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Benchmarking query performance across different configurations...\n"
     ]
    }
   ],
   "source": [
    "# Benchmark query performance\n",
    "print(\"\\nBenchmarking query performance across different configurations...\")\n",
    "\n",
    "# Prepare queries\n",
    "query_texts = [\n",
    "    \"Latest technology trends in artificial intelligence\",\n",
    "    \"Scientific research on climate change\",\n",
    "    \"Health benefits of regular exercise\",\n",
    "    \"Business strategies for startups\",\n",
    "    \"Entertainment news about recent movie releases\"\n",
    "]\n",
    "\n",
    "# Set up benchmark parameters\n",
    "results = {}\n",
    "num_trials = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing default configuration:\n",
      "  Query: 'Latest technology trends in ar...': 0.0144 seconds\n",
      "  Query: 'Scientific research on climate...': 0.0120 seconds\n",
      "  Query: 'Health benefits of regular exe...': 0.0122 seconds\n",
      "  Query: 'Business strategies for startu...': 0.0127 seconds\n",
      "  Query: 'Entertainment news about recen...': 0.0122 seconds\n",
      "\n",
      "Testing high_accuracy configuration:\n",
      "  Query: 'Latest technology trends in ar...': 0.0138 seconds\n",
      "  Query: 'Scientific research on climate...': 0.0123 seconds\n",
      "  Query: 'Health benefits of regular exe...': 0.0119 seconds\n",
      "  Query: 'Business strategies for startu...': 0.0126 seconds\n",
      "  Query: 'Entertainment news about recen...': 0.0119 seconds\n",
      "\n",
      "Testing fast_search configuration:\n",
      "  Query: 'Latest technology trends in ar...': 0.0118 seconds\n",
      "  Query: 'Scientific research on climate...': 0.0119 seconds\n",
      "  Query: 'Health benefits of regular exe...': 0.0117 seconds\n",
      "  Query: 'Business strategies for startu...': 0.0116 seconds\n",
      "  Query: 'Entertainment news about recen...': 0.0117 seconds\n"
     ]
    }
   ],
   "source": [
    "# Run benchmark for each collection\n",
    "for name, collection in collections.items():\n",
    "    print(f\"\\nTesting {name} configuration:\")\n",
    "    times = []\n",
    "    \n",
    "    for query in query_texts:\n",
    "        query_times = []\n",
    "        \n",
    "        for _ in range(num_trials):\n",
    "            start_time = time.time()\n",
    "            collection.query(\n",
    "                query_texts=[query],\n",
    "                n_results=10\n",
    "            )\n",
    "            query_time = time.time() - start_time\n",
    "            query_times.append(query_time)\n",
    "        \n",
    "        avg_time = sum(query_times) / len(query_times)\n",
    "        times.append(avg_time)\n",
    "        print(f\"  Query: '{query[:30]}...': {avg_time:.4f} seconds\")\n",
    "    \n",
    "    results[name] = {\n",
    "        \"mean\": sum(times) / len(times),\n",
    "        \"min\": min(times),\n",
    "        \"max\": max(times),\n",
    "        \"times\": times\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performance Summary:\n",
      "  default: Mean=0.0127s, Min=0.0120s, Max=0.0144s\n",
      "  high_accuracy: Mean=0.0125s, Min=0.0119s, Max=0.0138s\n",
      "  fast_search: Mean=0.0117s, Min=0.0116s, Max=0.0119s\n"
     ]
    }
   ],
   "source": [
    "# Print summary of benchmark results\n",
    "print(\"\\nPerformance Summary:\")\n",
    "for name, metrics in results.items():\n",
    "    print(f\"  {name}: Mean={metrics['mean']:.4f}s, Min={metrics['min']:.4f}s, Max={metrics['max']:.4f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Caching Implementation\n",
    "\n",
    "Caching is a crucial optimization technique for production systems. It can significantly reduce latency and computational load by storing frequently accessed results.\n",
    "\n",
    "### Why Implement Caching?\n",
    "\n",
    "1. **Reduced Latency** - Cached results can be returned instantly without computing embeddings or searching the vector space\n",
    "2. **Lower Computational Costs** - Fewer embedding calculations mean lower GPU/CPU usage\n",
    "3. **Better Scalability** - Handle more queries with the same resources\n",
    "\n",
    "We'll implement a simple LRU (Least Recently Used) cache and measure its performance impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CACHING IMPLEMENTATION ===\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "import time\n",
    "import random\n",
    "\n",
    "print(\"\\n=== CACHING IMPLEMENTATION ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LRU Cache Implementation\n",
    "\n",
    "Let's implement a simple LRU (Least Recently Used) cache. This type of cache keeps track of which queries are used most frequently and evicts the least recently used entries when the cache is full."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a simple LRU cache\n",
    "class LRUCache:\n",
    "    def __init__(self, capacity=100):\n",
    "        self.capacity = capacity  # Maximum number of items the cache can hold\n",
    "        self.cache = {}           # Dictionary to store cache items\n",
    "        self.usage_order = []     # List to track access order\n",
    "    \n",
    "    def get(self, key):\n",
    "        if key in self.cache:\n",
    "            # Update usage order - move to end of list (most recently used)\n",
    "            self.usage_order.remove(key)\n",
    "            self.usage_order.append(key)\n",
    "            return self.cache[key]\n",
    "        return None  # Cache miss\n",
    "    \n",
    "    def put(self, key, value):\n",
    "        if key in self.cache:\n",
    "            # Update existing entry\n",
    "            self.cache[key] = value\n",
    "            self.usage_order.remove(key)\n",
    "            self.usage_order.append(key)\n",
    "        else:\n",
    "            # Add new entry\n",
    "            if len(self.cache) >= self.capacity:\n",
    "                # Evict least recently used item\n",
    "                lru_key = self.usage_order.pop(0)\n",
    "                del self.cache[lru_key]\n",
    "            \n",
    "            self.cache[key] = value\n",
    "            self.usage_order.append(key)\n",
    "    \n",
    "    def clear(self):\n",
    "        self.cache = {}\n",
    "        self.usage_order = []\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up the Collection\n",
    "\n",
    "Now let's create a collection and populate it with sample documents for our caching experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Chroma\n",
    "client = chromadb.Client()\n",
    "embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "    model_name=\"all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "# Create a collection\n",
    "collection = client.create_collection(\n",
    "    name=\"cache_test\",\n",
    "    embedding_function=embedding_function\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 1000 documents to the collection\n"
     ]
    }
   ],
   "source": [
    "# Add sample documents\n",
    "num_docs = 1000\n",
    "documents = [f\"This is a sample document {i} with various content for testing caching\" for i in range(num_docs)]\n",
    "ids = [f\"cache_doc_{i}\" for i in range(num_docs)]\n",
    "\n",
    "# Add documents in batches to avoid overwhelming the system\n",
    "for i in range(0, num_docs, 100):\n",
    "    end_idx = min(i + 100, num_docs)\n",
    "    \n",
    "    collection.add(\n",
    "        documents=documents[i:end_idx],\n",
    "        ids=ids[i:end_idx]\n",
    "    )\n",
    "\n",
    "print(f\"Added {num_docs} documents to the collection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cached Query Function\n",
    "\n",
    "Let's implement a function that uses our cache to store and retrieve query results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize cache with a capacity of 50 entries\n",
    "query_cache = LRUCache(capacity=50)\n",
    "\n",
    "# Function to query with caching\n",
    "def cached_query(query_text, n_results=10, use_cache=True):\n",
    "    # Create a unique cache key from the query text and number of results\n",
    "    cache_key = f\"{query_text}:{n_results}\"\n",
    "    \n",
    "    if use_cache:\n",
    "        # Check cache first\n",
    "        cached_result = query_cache.get(cache_key)\n",
    "        if cached_result is not None:\n",
    "            return cached_result, True  # Cache hit\n",
    "    \n",
    "    # Cache miss or cache disabled, perform actual query\n",
    "    result = collection.query(\n",
    "        query_texts=[query_text],\n",
    "        n_results=n_results\n",
    "    )\n",
    "    \n",
    "    if use_cache:\n",
    "        # Update cache\n",
    "        query_cache.put(cache_key, result)\n",
    "    \n",
    "    return result, False  # Cache miss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Query Mix\n",
    "\n",
    "To simulate a realistic workload, we'll create a mix of common (frequently repeated) and unique queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing query performance with caching:\n"
     ]
    }
   ],
   "source": [
    "# Test queries with varying cache hit rates\n",
    "print(\"\\nTesting query performance with caching:\")\n",
    "\n",
    "# Prepare query mix (some repeated, some unique)\n",
    "common_queries = [\n",
    "    \"document with content\",\n",
    "    \"sample document\",\n",
    "    \"testing caching\",\n",
    "    \"various content\"\n",
    "]\n",
    "\n",
    "unique_queries = [f\"unique query {i}\" for i in range(50)]\n",
    "\n",
    "# Mix queries with different distributions to test cache performance\n",
    "mixed_queries = []\n",
    "for _ in range(20):\n",
    "    # Add common queries (higher probability)\n",
    "    mixed_queries.extend(common_queries)\n",
    "    \n",
    "    # Add some unique queries\n",
    "    mixed_queries.extend(random.sample(unique_queries, 5))\n",
    "\n",
    "# Shuffle to ensure realistic query pattern\n",
    "random.shuffle(mixed_queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark: No Cache vs. With Cache\n",
    "\n",
    "Now let's measure the performance difference between running queries without a cache versus with a cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running queries without cache...\n"
     ]
    }
   ],
   "source": [
    "# Run without cache\n",
    "print(\"Running queries without cache...\")\n",
    "start_time = time.time()\n",
    "\n",
    "for query in mixed_queries:\n",
    "    _, _ = cached_query(query, use_cache=False)\n",
    "\n",
    "no_cache_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running queries with cache...\n"
     ]
    }
   ],
   "source": [
    "# Run with cache\n",
    "print(\"Running queries with cache...\")\n",
    "query_cache.clear()  # Clear the cache\n",
    "\n",
    "start_time = time.time()\n",
    "hits = 0\n",
    "\n",
    "for query in mixed_queries:\n",
    "    _, is_hit = cached_query(query, use_cache=True)\n",
    "    if is_hit:\n",
    "        hits += 1\n",
    "\n",
    "with_cache_time = time.time() - start_time\n",
    "hit_rate = hits / len(mixed_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cache Performance Results:\n",
      "  Without cache: 2.1237 seconds\n",
      "  With cache: 0.5426 seconds\n",
      "  Time saved: 1.5811 seconds (74.5%)\n",
      "  Cache hit rate: 74.4%\n",
      "  Cache size: 46\n"
     ]
    }
   ],
   "source": [
    "# Report results\n",
    "print(\"\\nCache Performance Results:\")\n",
    "print(f\"  Without cache: {no_cache_time:.4f} seconds\")\n",
    "print(f\"  With cache: {with_cache_time:.4f} seconds\")\n",
    "print(f\"  Time saved: {no_cache_time - with_cache_time:.4f} seconds ({(1 - with_cache_time/no_cache_time) * 100:.1f}%)\")\n",
    "print(f\"  Cache hit rate: {hit_rate:.1%}\")\n",
    "print(f\"  Cache size: {len(query_cache)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Key Takeaways\n",
    "\n",
    "In this notebook, we've explored practical approaches to scaling vector databases for production use using ANNs:\n",
    "\n",
    "**ANN Implementations**\n",
    "   - Configuring HNSW parameters allows for customized speed-accuracy tradeoffs\n",
    "   - The right configuration depends on your specific application requirements"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
