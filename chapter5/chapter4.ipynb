{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling Vector Databases\n",
    "\n",
    "As we think about going to production, there are multiple considerations we need to keep top of mind:\n",
    "1. Speed vs. Accuracy\n",
    "2. Resource Limitations\n",
    "3. Horizontal Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ANN IMPLEMENTATION ===\n",
      "Generating 5000 sample documents...\n",
      "Adding documents to collections with different index configurations...\n",
      "  Added 5000 documents to default collection\n",
      "  Added 5000 documents to high_accuracy collection\n",
      "  Added 5000 documents to fast_search collection\n",
      "\n",
      "Benchmarking query performance across different configurations...\n",
      "\n",
      "Testing default configuration:\n",
      "  Query: 'Latest technology trends in ar...': 0.0122 seconds\n",
      "  Query: 'Scientific research on climate...': 0.0117 seconds\n",
      "  Query: 'Health benefits of regular exe...': 0.0115 seconds\n",
      "  Query: 'Business strategies for startu...': 0.0121 seconds\n",
      "  Query: 'Entertainment news about recen...': 0.0116 seconds\n",
      "\n",
      "Testing high_accuracy configuration:\n",
      "  Query: 'Latest technology trends in ar...': 0.0116 seconds\n",
      "  Query: 'Scientific research on climate...': 0.0115 seconds\n",
      "  Query: 'Health benefits of regular exe...': 0.0118 seconds\n",
      "  Query: 'Business strategies for startu...': 0.0118 seconds\n",
      "  Query: 'Entertainment news about recen...': 0.0116 seconds\n",
      "\n",
      "Testing fast_search configuration:\n",
      "  Query: 'Latest technology trends in ar...': 0.0114 seconds\n",
      "  Query: 'Scientific research on climate...': 0.0116 seconds\n",
      "  Query: 'Health benefits of regular exe...': 0.0120 seconds\n",
      "  Query: 'Business strategies for startu...': 0.0114 seconds\n",
      "  Query: 'Entertainment news about recen...': 0.0114 seconds\n",
      "\n",
      "Performance Summary:\n",
      "  default: Mean=0.0118s, Min=0.0115s, Max=0.0122s\n",
      "  high_accuracy: Mean=0.0117s, Min=0.0115s, Max=0.0118s\n",
      "  fast_search: Mean=0.0116s, Min=0.0114s, Max=0.0120s\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "import time\n",
    "\n",
    "client = chromadb.Client()\n",
    "embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "  model_name=\"all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "# Create collections with different HNSW configurations\n",
    "collections = {}\n",
    "\n",
    "print(\"\\n=== ANN IMPLEMENTATION ===\")\n",
    "\n",
    "# 1. Default settings\n",
    "collections[\"default\"] = client.create_collection(\n",
    "    name=\"default_index\",\n",
    "    embedding_function=embedding_function\n",
    ")\n",
    "\n",
    "# 2. High accuracy configuration\n",
    "collections[\"high_accuracy\"] = client.create_collection(\n",
    "    name=\"high_accuracy_index\",\n",
    "    embedding_function=embedding_function,\n",
    "    metadata={\"hnsw:space\": \"cosine\", \"hnsw:construction_ef\": 500, \"hnsw:search_ef\": 250, \"hnsw:M\": 36}\n",
    ")\n",
    "\n",
    "# 3. Fast search configuration\n",
    "collections[\"fast_search\"] = client.create_collection(\n",
    "    name=\"fast_search_index\",\n",
    "    embedding_function=embedding_function,\n",
    "    metadata={\"hnsw:space\": \"cosine\", \"hnsw:construction_ef\": 80, \"hnsw:search_ef\": 40, \"hnsw:M\": 12}\n",
    ")\n",
    "\n",
    "# Generate sample data\n",
    "num_docs = 5000\n",
    "print(f\"Generating {num_docs} sample documents...\")\n",
    "\n",
    "# Create documents with some patterns for testing\n",
    "categories = [\"technology\", \"science\", \"health\", \"business\", \"entertainment\"]\n",
    "documents = []\n",
    "ids = []\n",
    "\n",
    "for i in range(num_docs):\n",
    "    category = categories[i % len(categories)]\n",
    "    document = f\"This is document {i} about {category} with some additional text to make it more unique.\"\n",
    "    documents.append(document)\n",
    "    ids.append(f\"doc_{i}\")\n",
    "\n",
    "# Add documents to all collections\n",
    "print(\"Adding documents to collections with different index configurations...\")\n",
    "for name, collection in collections.items():\n",
    "    collection.add(\n",
    "        documents=documents,\n",
    "        ids=ids\n",
    "    )\n",
    "    print(f\"  Added {num_docs} documents to {name} collection\")\n",
    "\n",
    "# Benchmark query performance\n",
    "print(\"\\nBenchmarking query performance across different configurations...\")\n",
    "\n",
    "# Prepare queries\n",
    "query_texts = [\n",
    "    \"Latest technology trends in artificial intelligence\",\n",
    "    \"Scientific research on climate change\",\n",
    "    \"Health benefits of regular exercise\",\n",
    "    \"Business strategies for startups\",\n",
    "    \"Entertainment news about recent movie releases\"\n",
    "]\n",
    "\n",
    "# Run benchmark\n",
    "results = {}\n",
    "num_trials = 5\n",
    "\n",
    "for name, collection in collections.items():\n",
    "    print(f\"\\nTesting {name} configuration:\")\n",
    "    times = []\n",
    "    \n",
    "    for query in query_texts:\n",
    "        query_times = []\n",
    "        \n",
    "        for _ in range(num_trials):\n",
    "            start_time = time.time()\n",
    "            collection.query(\n",
    "                query_texts=[query],\n",
    "                n_results=10\n",
    "            )\n",
    "            query_time = time.time() - start_time\n",
    "            query_times.append(query_time)\n",
    "        \n",
    "        avg_time = sum(query_times) / len(query_times)\n",
    "        times.append(avg_time)\n",
    "        print(f\"  Query: '{query[:30]}...': {avg_time:.4f} seconds\")\n",
    "    \n",
    "    results[name] = {\n",
    "        \"mean\": sum(times) / len(times),\n",
    "        \"min\": min(times),\n",
    "        \"max\": max(times),\n",
    "        \"times\": times\n",
    "    }\n",
    "\n",
    "# Print summary\n",
    "print(\"\\nPerformance Summary:\")\n",
    "for name, metrics in results.items():\n",
    "    print(f\"  {name}: Mean={metrics['mean']:.4f}s, Min={metrics['min']:.4f}s, Max={metrics['max']:.4f}s\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CACHING IMPLEMENTATION ===\n",
      "Added 1000 documents to the collection\n",
      "\n",
      "Testing query performance with caching:\n",
      "Running queries without cache...\n",
      "Running queries with cache...\n",
      "\n",
      "Cache Performance Results:\n",
      "  Without cache: 2.1211 seconds\n",
      "  With cache: 0.6190 seconds\n",
      "  Time saved: 1.5022 seconds (70.8%)\n",
      "  Cache hit rate: 71.7%\n",
      "  Cache size: 50\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "import time\n",
    "import random\n",
    "\n",
    "print(\"\\n=== CACHING IMPLEMENTATION ===\")\n",
    "\n",
    "# Implement a simple LRU cache\n",
    "class LRUCache:\n",
    "    def __init__(self, capacity=100):\n",
    "        self.capacity = capacity\n",
    "        self.cache = {}\n",
    "        self.usage_order = []\n",
    "    \n",
    "    def get(self, key):\n",
    "        if key in self.cache:\n",
    "            # Update usage order\n",
    "            self.usage_order.remove(key)\n",
    "            self.usage_order.append(key)\n",
    "            return self.cache[key]\n",
    "        return None\n",
    "    \n",
    "    def put(self, key, value):\n",
    "        if key in self.cache:\n",
    "            # Update existing entry\n",
    "            self.cache[key] = value\n",
    "            self.usage_order.remove(key)\n",
    "            self.usage_order.append(key)\n",
    "        else:\n",
    "            # Add new entry\n",
    "            if len(self.cache) >= self.capacity:\n",
    "                # Evict least recently used\n",
    "                lru_key = self.usage_order.pop(0)\n",
    "                del self.cache[lru_key]\n",
    "            \n",
    "            self.cache[key] = value\n",
    "            self.usage_order.append(key)\n",
    "    \n",
    "    def clear(self):\n",
    "        self.cache = {}\n",
    "        self.usage_order = []\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.cache)\n",
    "\n",
    "\n",
    "# Initialize Chroma\n",
    "client = chromadb.Client()\n",
    "embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "    model_name=\"all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "# Create a collection\n",
    "collection = client.create_collection(\n",
    "    name=\"cache_test\",\n",
    "    embedding_function=embedding_function\n",
    ")\n",
    "\n",
    "# Add sample documents\n",
    "num_docs = 1000\n",
    "documents = [f\"This is a sample document {i} with various content for testing caching\" for i in range(num_docs)]\n",
    "ids = [f\"cache_doc_{i}\" for i in range(num_docs)]\n",
    "\n",
    "for i in range(0, num_docs, 100):\n",
    "    end_idx = min(i + 100, num_docs)\n",
    "    \n",
    "    collection.add(\n",
    "        documents=documents[i:end_idx],\n",
    "        ids=ids[i:end_idx]\n",
    "    )\n",
    "\n",
    "print(f\"Added {num_docs} documents to the collection\")\n",
    "\n",
    "# Initialize cache\n",
    "query_cache = LRUCache(capacity=50)\n",
    "\n",
    "# Function to query with caching\n",
    "def cached_query(query_text, n_results=10, use_cache=True):\n",
    "    cache_key = f\"{query_text}:{n_results}\"\n",
    "    \n",
    "    if use_cache:\n",
    "        # Check cache first\n",
    "        cached_result = query_cache.get(cache_key)\n",
    "        if cached_result is not None:\n",
    "            return cached_result, True  # Cache hit\n",
    "    \n",
    "    # Cache miss or cache disabled, perform actual query\n",
    "    result = collection.query(\n",
    "        query_texts=[query_text],\n",
    "        n_results=n_results\n",
    "    )\n",
    "    \n",
    "    if use_cache:\n",
    "        # Update cache\n",
    "        query_cache.put(cache_key, result)\n",
    "    \n",
    "    return result, False  # Cache miss\n",
    "\n",
    "# Test queries with varying cache hit rates\n",
    "print(\"\\nTesting query performance with caching:\")\n",
    "\n",
    "# Prepare query mix (some repeated, some unique)\n",
    "common_queries = [\n",
    "    \"document with content\",\n",
    "    \"sample document\",\n",
    "    \"testing caching\",\n",
    "    \"various content\"\n",
    "]\n",
    "\n",
    "unique_queries = [f\"unique query {i}\" for i in range(50)]\n",
    "\n",
    "# Mix queries with different distributions to test cache performance\n",
    "mixed_queries = []\n",
    "for _ in range(20):\n",
    "    # Add common queries (higher probability)\n",
    "    mixed_queries.extend(common_queries)\n",
    "    \n",
    "    # Add some unique queries\n",
    "    mixed_queries.extend(random.sample(unique_queries, 5))\n",
    "\n",
    "random.shuffle(mixed_queries)\n",
    "\n",
    "# Run without cache\n",
    "print(\"Running queries without cache...\")\n",
    "start_time = time.time()\n",
    "\n",
    "for query in mixed_queries:\n",
    "    _, _ = cached_query(query, use_cache=False)\n",
    "\n",
    "no_cache_time = time.time() - start_time\n",
    "\n",
    "# Run with cache\n",
    "print(\"Running queries with cache...\")\n",
    "query_cache.clear()  # Clear the cache\n",
    "\n",
    "start_time = time.time()\n",
    "hits = 0\n",
    "\n",
    "for query in mixed_queries:\n",
    "    _, is_hit = cached_query(query, use_cache=True)\n",
    "    if is_hit:\n",
    "        hits += 1\n",
    "\n",
    "with_cache_time = time.time() - start_time\n",
    "hit_rate = hits / len(mixed_queries)\n",
    "\n",
    "# Report results\n",
    "print(\"\\nCache Performance Results:\")\n",
    "print(f\"  Without cache: {no_cache_time:.4f} seconds\")\n",
    "print(f\"  With cache: {with_cache_time:.4f} seconds\")\n",
    "print(f\"  Time saved: {no_cache_time - with_cache_time:.4f} seconds ({(1 - with_cache_time/no_cache_time) * 100:.1f}%)\")\n",
    "print(f\"  Cache hit rate: {hit_rate:.1%}\")\n",
    "print(f\"  Cache size: {len(query_cache)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
