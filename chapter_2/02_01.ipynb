{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d92754a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.12.9 environment at: /workspaces/fundamentals-of-ai-engineering-principles-and-practical-applications-6026542/.venv\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 119ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb2d4d57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50ec64c770304d7b84aa5e94f743c70a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/3.69k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f50ceb904fc4c3289d7e47611ed042e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/801k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6d912c8e19e40898e9d7074e0712765",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83e7838651f744b7b476d76db71315d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.10M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "404927bea5a4456fb13644c80d066175",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/831 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7d48165a9be4ed3a28b811531e2bbd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/724 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e898b97bea0148a092a0922972f549c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/538M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b41040c912974d1fa5e648e9178fab2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def print_hello_world():\n",
      "    print(\"Hello World!\")\n",
      "\n",
      "# print_hello_world()\n",
      "\n",
      "# print_\n"
     ]
    }
   ],
   "source": [
    "# pip install transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "checkpoint = \"HuggingFaceTB/SmolLM-135M\"\n",
    "device = \"cpu\"  # for GPU usage or \"cpu\" for CPU usage\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "# for multiple GPUs install accelerate and do `model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\")`\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)\n",
    "inputs = tokenizer.encode(\"def print_hello_world():\",\n",
    "                          return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(inputs)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d077d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA not available. Running on CPU which will be slow.\n",
      "For better performance, consider using Google Colab or a machine with a GPU.\n",
      "Loading TinyLlama-1.1B model - one of the smallest practical LLMs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59ef2ddf35ab47088e4fe4095d2f6589",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5041b1f074545daa8e1cf6787823a5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "Running on: cpu\n",
      "\n",
      "==================================================\n",
      "DEMONSTRATION OF TINY LOCAL LLM CAPABILITIES\n",
      "==================================================\n",
      "\n",
      "Example 1: \"What is machine learning?\"\n",
      "----------------------------------------\n",
      "Response: Machine learning is a field of artificial intelligence that involves the use of algorithms to learn and improve from data. In simple terms, machine learning is the process of using data to teach a computer to perform tasks that it was not explicitly designed to perform. It involves developing algorithms that can learn from data without being explicitly programmed, and then using these algorithms to make predictions, recommendations, or decisions.\n",
      "Some examples of machine learning algorithms include:\n",
      "1. Decision trees: This algorithm is used\n",
      "----------------------------------------\n",
      "\n",
      "Example 2: \"Write a short poem about artificial intelligence.\"\n",
      "----------------------------------------\n",
      "Response: A world of wonder and new discoveries,\n",
      "A future of infinite possibilities,\n",
      "Where machines can think and reason like humans,\n",
      "And unlock the secrets of the universe.\n",
      "\n",
      "The machines are learning and growing,\n",
      "Filling the world with knowledge and wisdom,\n",
      "As they help us in every way,\n",
      "Transforming the way we see the world.\n",
      "\n",
      "Their algorithms and neural networks,\n",
      "Bringing us closer to the truth,\n",
      "Adapting to\n",
      "----------------------------------------\n",
      "\n",
      "Example 3: \"Explain how to make a cup of coffee in 3 steps.\"\n",
      "----------------------------------------\n",
      "Response: Coffee making is a simple process that can be done in three steps:\n",
      "\n",
      "1. Grind the coffee beans: Use a grinder to grind the coffee beans to the right consistency. Beans with a finer grind will make a richer and more flavorful cup of coffee.\n",
      "\n",
      "2. Steep the coffee: Fill a mug with hot water and add the coffee beans. Be sure to keep the water at a temperature between\n",
      "----------------------------------------\n",
      "\n",
      "==================================================\n",
      "INTERACTIVE MODE - Type 'exit' to quit\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Simple Local LLM Demo (Fixed Version)\n",
    "# For LinkedIn Learning course chapter on running local LLMs\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "def run_tiny_llm():\n",
    "    print(\"Loading TinyLlama-1.1B model - one of the smallest practical LLMs...\")\n",
    "\n",
    "    # Load the model and tokenizer\n",
    "    # TinyLlama-1.1B is about 1.1B parameters - very small for an LLM\n",
    "    model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # Load model with minimal settings (no quantization)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16 if torch.cuda.is_available() else None,\n",
    "        device_map=\"auto\" if torch.cuda.is_available() else None\n",
    "    )\n",
    "\n",
    "    # Move to device\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    if device == \"cpu\":\n",
    "        model = model.to(device)\n",
    "\n",
    "    print(f\"Model loaded successfully!\")\n",
    "    print(f\"Running on: {next(model.parameters()).device}\")\n",
    "\n",
    "    # Function to generate text\n",
    "    def generate_response(prompt, max_length=100):\n",
    "        # Format the prompt according to the model's expected format\n",
    "        formatted_prompt = f\"<|user|>\\n{prompt}\\n<|assistant|>\\n\"\n",
    "\n",
    "        # Tokenize the prompt\n",
    "        inputs = tokenizer(\n",
    "            formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        # Generate\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_length,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                do_sample=True\n",
    "            )\n",
    "\n",
    "        # Decode and return\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        # Extract just the assistant's response\n",
    "        response = response.split(\"<|assistant|>\")[-1].strip()\n",
    "        return response\n",
    "\n",
    "    # Demo the model with a few examples\n",
    "    examples = [\n",
    "        \"What is machine learning?\",\n",
    "        \"Write a short poem about artificial intelligence.\",\n",
    "        \"Explain how to make a cup of coffee in 3 steps.\"\n",
    "    ]\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"DEMONSTRATION OF TINY LOCAL LLM CAPABILITIES\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    for i, example in enumerate(examples, 1):\n",
    "        print(f\"\\nExample {i}: \\\"{example}\\\"\")\n",
    "        print(\"-\" * 40)\n",
    "        response = generate_response(example)\n",
    "        print(f\"Response: {response}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "    # Interactive mode\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"INTERACTIVE MODE - Type 'exit' to quit\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"\\nEnter your prompt: \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            break\n",
    "\n",
    "        response = generate_response(user_input)\n",
    "        print(f\"\\nResponse: {response}\")\n",
    "\n",
    "    print(\"\\nDemo completed!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Check if CUDA is available\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"CUDA available: {torch.cuda.get_device_name(0)}\")\n",
    "    else:\n",
    "        print(\"CUDA not available. Running on CPU which will be slow.\")\n",
    "        print(\n",
    "            \"For better performance, consider using Google Colab or a machine with a GPU.\")\n",
    "\n",
    "    run_tiny_llm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab023630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading distilbert/distilgpt2...\n",
      "Model loaded successfully!\n",
      "\n",
      "DEMONSTRATION OF TINY LLM\n",
      "\n",
      "Prompt: \"Artificial intelligence is\"\n",
      "Generated: Artificial intelligence is one of the most powerful tools in recent history. It can detect and classify information in the form of pictures, videos, pictures, pictures, images or any other information. It can also pick out patterns of behavior and predict what to\n",
      "\n",
      "Prompt: \"The future of technology\"\n",
      "Generated: The future of technology is uncertain, as well as the potential for other forms of information disruption.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Prompt: \"Learning to code\"\n",
      "Generated: Learning to code and make life a more livable environment.\n"
     ]
    }
   ],
   "source": [
    "# Fixed Minimal LLM Demo - CPU Only\n",
    "# For LinkedIn Learning course on local LLMs\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load the smallest practical model (DistilGPT2 - 15M parameters)\n",
    "model_name = \"distilbert/distilgpt2\"\n",
    "print(f\"Loading {model_name}...\")\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Fix: Set proper padding token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# Fixed function to generate text with proper attention mask\n",
    "\n",
    "\n",
    "def generate_text(prompt, max_length=50):\n",
    "    # Encode the input with attention mask\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        return_attention_mask=True\n",
    "    )\n",
    "\n",
    "    # Generate with proper parameters\n",
    "    output = model.generate(\n",
    "        input_ids=inputs.input_ids,\n",
    "        attention_mask=inputs.attention_mask,\n",
    "        max_length=max_length,\n",
    "        do_sample=True,\n",
    "        temperature=0.8,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "\n",
    "    # Decode and return\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# Example prompts\n",
    "examples = [\n",
    "    \"Artificial intelligence is\",\n",
    "    \"The future of technology\",\n",
    "    \"Learning to code\"\n",
    "]\n",
    "\n",
    "print(\"\\nDEMONSTRATION OF TINY LLM\")\n",
    "\n",
    "# Run examples\n",
    "for example in examples:\n",
    "    print(f\"\\nPrompt: \\\"{example}\\\"\")\n",
    "    response = generate_text(example)\n",
    "    print(f\"Generated: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93039757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: \"Artificial intelligence is\"\n",
      "Generated: Artificial intelligence is a topic that has been hotly debated. But, as such, we're not the only ones that want to learn.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"distilbert/distilgpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Set padding token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Simple generation function\n",
    "\n",
    "\n",
    "def generate_text(prompt, max_length=50):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", return_attention_mask=True)\n",
    "    output = model.generate(\n",
    "        input_ids=inputs.input_ids,\n",
    "        attention_mask=inputs.attention_mask,\n",
    "        max_length=max_length,\n",
    "        do_sample=True,\n",
    "        temperature=0.8,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "prompt = \"Artificial intelligence is\"\n",
    "response = generate_text(prompt)\n",
    "print(f\"Prompt: \\\"{prompt}\\\"\")\n",
    "print(f\"Generated: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e2506a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to ./saved_model...\n",
      "Model and tokenizer saved successfully!\n",
      "Loading model from local directory...\n",
      "Model loaded from local directory!\n",
      "Prompt: \"Artificial intelligence is\"\n",
      "Generated: Artificial intelligence is not merely a tool for solving problems but an important tool to help solve them. It is a powerful tool that can lead to meaningful and lasting results for both humans and people.\n",
      "\n",
      "\n",
      "To learn about Artificial Intelligence, please visit\n"
     ]
    }
   ],
   "source": [
    "# Import only what's needed\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "\n",
    "# Set the directory where you want to save the model\n",
    "save_directory = \"./saved_model\"  # Change this to your preferred path\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"distilbert/distilgpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Set padding token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Save the model and tokenizer to the specified directory\n",
    "print(f\"Saving model to {save_directory}...\")\n",
    "model.save_pretrained(save_directory)\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "print(\"Model and tokenizer saved successfully!\")\n",
    "\n",
    "# Now we can load from local directory instead of downloading again\n",
    "print(\"Loading model from local directory...\")\n",
    "local_model = AutoModelForCausalLM.from_pretrained(save_directory)\n",
    "local_tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
    "local_tokenizer.pad_token = local_tokenizer.eos_token\n",
    "print(\"Model loaded from local directory!\")\n",
    "\n",
    "# Simple generation function using the local model\n",
    "\n",
    "\n",
    "def generate_text(prompt, max_length=50):\n",
    "    inputs = local_tokenizer(prompt, return_tensors=\"pt\",\n",
    "                             return_attention_mask=True)\n",
    "    output = local_model.generate(\n",
    "        input_ids=inputs.input_ids,\n",
    "        attention_mask=inputs.attention_mask,\n",
    "        max_length=max_length,\n",
    "        do_sample=True,\n",
    "        temperature=0.8,\n",
    "        pad_token_id=local_tokenizer.pad_token_id\n",
    "    )\n",
    "    return local_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "prompt = \"Artificial intelligence is\"\n",
    "response = generate_text(prompt)\n",
    "print(f\"Prompt: \\\"{prompt}\\\"\")\n",
    "print(f\"Generated: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c448d4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e81bcd7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: Artificial intelligence will\n",
      "Token IDs: [8001, 9542, 4430, 481]\n",
      "Individual tokens: ['Art', 'ificial', ' intelligence', ' will']\n",
      "Model input structure: dict_keys(['input_ids', 'attention_mask'])\n",
      "Input shape: torch.Size([1, 4])\n",
      "Generated text: Artificial intelligence will be a significant step towards a future where companies are able to develop and manage AI. Artificial intelligence will be at a high level that we are already seeing.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Breaking down the inference process step by step\n",
    "\n",
    "# Step 1: Load tokenizer and model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"distilbert/distilgpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Step 2: Prepare input text and convert to tokens\n",
    "input_text = \"Artificial intelligence will\"\n",
    "print(f\"Original text: {input_text}\")\n",
    "\n",
    "# Tokenization: Convert text to token IDs\n",
    "tokens = tokenizer.encode(input_text)\n",
    "print(f\"Token IDs: {tokens}\")\n",
    "\n",
    "# See the actual tokens\n",
    "tokens_decoded = [tokenizer.decode([token]) for token in tokens]\n",
    "print(f\"Individual tokens: {tokens_decoded}\")\n",
    "\n",
    "# Step 3: Prepare inputs for the model (add batch dimension and attention mask)\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", return_attention_mask=True)\n",
    "print(f\"Model input structure: {inputs.keys()}\")\n",
    "print(f\"Input shape: {inputs.input_ids.shape}\")\n",
    "\n",
    "# Step 4: Generate text (the actual inference)\n",
    "with torch.no_grad():  # Disable gradient calculation for inference\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs.input_ids,\n",
    "        attention_mask=inputs.attention_mask,\n",
    "        max_length=50,\n",
    "        do_sample=True,\n",
    "        temperature=0.8,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "# Step 5: Decode the generated tokens back to text\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"Generated text: {generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88f4bd5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: Artificial intelligence models like GPT-2 transform text into tokens.\n",
      "Tokenized: ['Art', 'ificial', 'Ġintelligence', 'Ġmodels', 'Ġlike', 'ĠG', 'PT', '-', '2', 'Ġtransform', 'Ġtext', 'Ġinto', 'Ġtokens', '.']\n",
      "Number of tokens: 14\n",
      "\n",
      "Special tokens in this tokenizer:\n",
      "BOS (beginning of sequence) token: <|endoftext|>, ID: 50256\n",
      "EOS (end of sequence) token: <|endoftext|>, ID: 50256\n",
      "PAD (padding) token: None, ID: None\n",
      "UNK (unknown) token: <|endoftext|>, ID: 50256\n",
      "\n",
      "Tokenizing unusual or new words:\n",
      "'cryptocurrencies' → ['crypt', 'oc', 'urrencies']\n",
      "'transformers' → ['transform', 'ers']\n",
      "'COVID19' → ['CO', 'VID', '19']\n",
      "'😊' → ['ðŁĺ', 'Ĭ']\n",
      "'anthropomorphization' → ['anthrop', 'omorph', 'ization']\n",
      "\n",
      "Long text token count: 54\n",
      "Maximum context length for this model: 1024\n",
      "\n",
      "Handling texts of different lengths:\n",
      "Text 1 length: 2 tokens\n",
      "Text 2 length: 9 tokens\n",
      "Text 3 length: 13 tokens\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 48\u001b[39m\n\u001b[32m     45\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mText \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m length: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m tokens\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# With padding and truncation\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m encoded_padded = \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mAfter padding/truncation to length 20:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     51\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInput shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mencoded_padded.input_ids.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/fundamentals-of-ai-engineering-principles-and-practical-applications-6026542/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2887\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.__call__\u001b[39m\u001b[34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[39m\n\u001b[32m   2885\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._in_target_context_manager:\n\u001b[32m   2886\u001b[39m         \u001b[38;5;28mself\u001b[39m._switch_to_input_mode()\n\u001b[32m-> \u001b[39m\u001b[32m2887\u001b[39m     encodings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2888\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2889\u001b[39m     \u001b[38;5;28mself\u001b[39m._switch_to_target_mode()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/fundamentals-of-ai-engineering-principles-and-practical-applications-6026542/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2975\u001b[39m, in \u001b[36mPreTrainedTokenizerBase._call_one\u001b[39m\u001b[34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[39m\n\u001b[32m   2970\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2971\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mbatch length of `text`: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not match batch length of `text_pair`:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2972\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text_pair)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2973\u001b[39m         )\n\u001b[32m   2974\u001b[39m     batch_text_or_text_pairs = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(text, text_pair)) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m text\n\u001b[32m-> \u001b[39m\u001b[32m2975\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbatch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2976\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2977\u001b[39m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2978\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2979\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2980\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2981\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2982\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2983\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2984\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2985\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2986\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2987\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2988\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2989\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2990\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2991\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2992\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2993\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2994\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2995\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2996\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2997\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.encode_plus(\n\u001b[32m   2998\u001b[39m         text=text,\n\u001b[32m   2999\u001b[39m         text_pair=text_pair,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3017\u001b[39m         **kwargs,\n\u001b[32m   3018\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/fundamentals-of-ai-engineering-principles-and-practical-applications-6026542/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3168\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.batch_encode_plus\u001b[39m\u001b[34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[39m\n\u001b[32m   3151\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3152\u001b[39m \u001b[33;03mTokenize and prepare for the model a list of sequences or a list of pairs of sequences.\u001b[39;00m\n\u001b[32m   3153\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   3164\u001b[39m \u001b[33;03m        details in `encode_plus`).\u001b[39;00m\n\u001b[32m   3165\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3167\u001b[39m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3168\u001b[39m padding_strategy, truncation_strategy, max_length, kwargs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_padding_truncation_strategies\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3169\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3170\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3171\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3172\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3173\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3174\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3175\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3177\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._batch_encode_plus(\n\u001b[32m   3178\u001b[39m     batch_text_or_text_pairs=batch_text_or_text_pairs,\n\u001b[32m   3179\u001b[39m     add_special_tokens=add_special_tokens,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3196\u001b[39m     **kwargs,\n\u001b[32m   3197\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/fundamentals-of-ai-engineering-principles-and-practical-applications-6026542/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2789\u001b[39m, in \u001b[36mPreTrainedTokenizerBase._get_padding_truncation_strategies\u001b[39m\u001b[34m(self, padding, truncation, max_length, pad_to_multiple_of, verbose, **kwargs)\u001b[39m\n\u001b[32m   2787\u001b[39m \u001b[38;5;66;03m# Test if we have a padding token\u001b[39;00m\n\u001b[32m   2788\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m padding_strategy != PaddingStrategy.DO_NOT_PAD \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.pad_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pad_token_id < \u001b[32m0\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m2789\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2790\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAsking to pad but the tokenizer does not have a padding token. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2791\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2792\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mor add a new pad token via `tokenizer.add_special_tokens(\u001b[39m\u001b[33m{\u001b[39m\u001b[33m'\u001b[39m\u001b[33mpad_token\u001b[39m\u001b[33m'\u001b[39m\u001b[33m: \u001b[39m\u001b[33m'\u001b[39m\u001b[33m[PAD]\u001b[39m\u001b[33m'\u001b[39m\u001b[33m})`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2793\u001b[39m     )\n\u001b[32m   2795\u001b[39m \u001b[38;5;66;03m# Check that we will truncate to a multiple of pad_to_multiple_of if both are provided\u001b[39;00m\n\u001b[32m   2796\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2797\u001b[39m     truncation_strategy != TruncationStrategy.DO_NOT_TRUNCATE\n\u001b[32m   2798\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m padding_strategy != PaddingStrategy.DO_NOT_PAD\n\u001b[32m   (...)\u001b[39m\u001b[32m   2801\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (max_length % pad_to_multiple_of != \u001b[32m0\u001b[39m)\n\u001b[32m   2802\u001b[39m ):\n",
      "\u001b[31mValueError\u001b[39m: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`."
     ]
    }
   ],
   "source": [
    "# Exploring tokenization in detail\n",
    "\n",
    "# 1. Different tokenization approaches\n",
    "input_text = \"Artificial intelligence models like GPT-2 transform text into tokens.\"\n",
    "\n",
    "# Show how the tokenizer breaks down text\n",
    "tokens = tokenizer.tokenize(input_text)\n",
    "print(f\"Input text: {input_text}\")\n",
    "print(f\"Tokenized: {tokens}\")\n",
    "print(f\"Number of tokens: {len(tokens)}\")\n",
    "\n",
    "# 2. Special tokens and their purpose\n",
    "print(f\"\\nSpecial tokens in this tokenizer:\")\n",
    "print(\n",
    "    f\"BOS (beginning of sequence) token: {tokenizer.bos_token}, ID: {tokenizer.bos_token_id}\")\n",
    "print(\n",
    "    f\"EOS (end of sequence) token: {tokenizer.eos_token}, ID: {tokenizer.eos_token_id}\")\n",
    "print(\n",
    "    f\"PAD (padding) token: {tokenizer.pad_token}, ID: {tokenizer.pad_token_id}\")\n",
    "print(\n",
    "    f\"UNK (unknown) token: {tokenizer.unk_token}, ID: {tokenizer.unk_token_id}\")\n",
    "\n",
    "# 3. Handling out-of-vocabulary words\n",
    "unusual_words = [\"cryptocurrencies\", \"transformers\",\n",
    "                 \"COVID19\", \"😊\", \"anthropomorphization\"]\n",
    "print(\"\\nTokenizing unusual or new words:\")\n",
    "for word in unusual_words:\n",
    "    word_tokens = tokenizer.tokenize(word)\n",
    "    print(f\"'{word}' → {word_tokens}\")\n",
    "\n",
    "# 4. Demonstrate context window with token counting\n",
    "long_text = \"This is \" + \"very \" * 50 + \"long.\"\n",
    "tokens = tokenizer.tokenize(long_text)\n",
    "print(f\"\\nLong text token count: {len(tokens)}\")\n",
    "print(\n",
    "    f\"Maximum context length for this model: {model.config.max_position_embeddings}\")\n",
    "\n",
    "# 5. Show the effect of truncation and padding\n",
    "texts = [\"Short text\", \"This is a bit longer text with more content\", \"A\" * 100]\n",
    "print(\"\\nHandling texts of different lengths:\")\n",
    "\n",
    "# Without padding/truncation\n",
    "encoded = [tokenizer.encode(t) for t in texts]\n",
    "for i, e in enumerate(encoded):\n",
    "    print(f\"Text {i+1} length: {len(e)} tokens\")\n",
    "\n",
    "# With padding and truncation\n",
    "encoded_padded = tokenizer(\n",
    "    texts, padding=True, truncation=True, max_length=20, return_tensors=\"pt\")\n",
    "print(f\"\\nAfter padding/truncation to length 20:\")\n",
    "print(f\"Input shape: {encoded_padded.input_ids.shape}\")\n",
    "print(f\"Attention mask: \\n{encoded_padded.attention_mask}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "236e48de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: Artificial intelligence models like GPT-2 transform text into tokens.\n",
      "Tokenized: ['Art', 'ificial', 'Ġintelligence', 'Ġmodels', 'Ġlike', 'ĠG', 'PT', '-', '2', 'Ġtransform', 'Ġtext', 'Ġinto', 'Ġtokens', '.']\n",
      "Number of tokens: 14\n",
      "\n",
      "Special tokens in this tokenizer:\n",
      "BOS (beginning of sequence) token: <|endoftext|>, ID: 50256\n",
      "EOS (end of sequence) token: <|endoftext|>, ID: 50256\n",
      "PAD (padding) token: <|endoftext|>, ID: 50256\n",
      "UNK (unknown) token: <|endoftext|>, ID: 50256\n",
      "\n",
      "Tokenizing unusual or new words:\n",
      "'cryptocurrencies' → ['crypt', 'oc', 'urrencies']\n",
      "'transformers' → ['transform', 'ers']\n",
      "'COVID19' → ['CO', 'VID', '19']\n",
      "'AI2023' → ['AI', '20', '23']\n",
      "'neural' → ['ne', 'ural']\n",
      "\n",
      "Converting tokens to IDs:\n",
      "Token IDs: [8001, 9542, 4430, 4981, 588, 402, 11571, 12, 17, 6121, 2420, 656, 16326, 13]\n",
      "\n",
      "Handling texts of different lengths:\n",
      "Text 1 length: 2 tokens\n",
      "Text 2 length: 9 tokens\n",
      "Text 3 length: 13 tokens\n",
      "\n",
      "After padding/truncation to length 20:\n",
      "Input shape: torch.Size([3, 13])\n",
      "Sample padded sequence: tensor([16438,  2420, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256])\n",
      "Attention mask (1=real token, 0=padding): \n",
      "tensor([[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "\n",
      "Original text: AI is changing the world\n",
      "Encoded and then decoded: AI is changing the world\n",
      "\n",
      "Subword tokenization examples:\n",
      "'unhappiness' → ['un', 'h', 'appiness']\n",
      "'multilingual' → ['mult', 'ilingual']\n",
      "'pretrained' → ['pret', 'rained']\n",
      "\n",
      "Long text example:\n",
      "Number of characters: 1359\n",
      "Number of tokens: 300\n",
      "First 10 tokens: ['This', 'Ġis', 'Ġparagraph', 'Ġ0', '.', 'ĠIt', 'Ġcontains', 'Ġsome', 'Ġtext', '.']\n",
      "Last 10 tokens: ['ĠIt', 'Ġcontains', 'Ġsome', 'Ġtext', '.', 'ĠIt', 'Ġcontains', 'Ġsome', 'Ġtext', '.']\n"
     ]
    }
   ],
   "source": [
    "# Exploring tokenization in detail\n",
    "\n",
    "# 1. Load the tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"distilbert/distilgpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Set padding token (required for batched operations)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 2. Different tokenization approaches\n",
    "input_text = \"Artificial intelligence models like GPT-2 transform text into tokens.\"\n",
    "\n",
    "# Show how the tokenizer breaks down text\n",
    "tokens = tokenizer.tokenize(input_text)\n",
    "print(f\"Input text: {input_text}\")\n",
    "print(f\"Tokenized: {tokens}\")\n",
    "print(f\"Number of tokens: {len(tokens)}\")\n",
    "\n",
    "# 3. Special tokens and their purpose\n",
    "print(f\"\\nSpecial tokens in this tokenizer:\")\n",
    "print(\n",
    "    f\"BOS (beginning of sequence) token: {tokenizer.bos_token}, ID: {tokenizer.bos_token_id}\")\n",
    "print(\n",
    "    f\"EOS (end of sequence) token: {tokenizer.eos_token}, ID: {tokenizer.eos_token_id}\")\n",
    "print(\n",
    "    f\"PAD (padding) token: {tokenizer.pad_token}, ID: {tokenizer.pad_token_id}\")\n",
    "print(\n",
    "    f\"UNK (unknown) token: {tokenizer.unk_token}, ID: {tokenizer.unk_token_id}\")\n",
    "\n",
    "# 4. Handling out-of-vocabulary words\n",
    "unusual_words = [\"cryptocurrencies\",\n",
    "                 \"transformers\", \"COVID19\", \"AI2023\", \"neural\"]\n",
    "print(\"\\nTokenizing unusual or new words:\")\n",
    "for word in unusual_words:\n",
    "    word_tokens = tokenizer.tokenize(word)\n",
    "    print(f\"'{word}' → {word_tokens}\")\n",
    "\n",
    "# 5. Demonstrate token IDs\n",
    "print(\"\\nConverting tokens to IDs:\")\n",
    "token_ids = tokenizer.encode(input_text)\n",
    "print(f\"Token IDs: {token_ids}\")\n",
    "\n",
    "# 6. Show the effect of truncation and padding\n",
    "texts = [\"Short text\", \"This is a bit longer text with more content\", \"A\" * 100]\n",
    "print(\"\\nHandling texts of different lengths:\")\n",
    "\n",
    "# Without padding/truncation - just show lengths\n",
    "encoded = [tokenizer.encode(t) for t in texts]\n",
    "for i, e in enumerate(encoded):\n",
    "    print(f\"Text {i+1} length: {len(e)} tokens\")\n",
    "\n",
    "# With padding and truncation\n",
    "encoded_padded = tokenizer(\n",
    "    texts,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=20,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "print(f\"\\nAfter padding/truncation to length 20:\")\n",
    "print(f\"Input shape: {encoded_padded.input_ids.shape}\")\n",
    "print(f\"Sample padded sequence: {encoded_padded.input_ids[0]}\")\n",
    "print(\n",
    "    f\"Attention mask (1=real token, 0=padding): \\n{encoded_padded.attention_mask}\")\n",
    "\n",
    "# 7. Decoding back to text\n",
    "original_text = \"AI is changing the world\"\n",
    "print(f\"\\nOriginal text: {original_text}\")\n",
    "\n",
    "# Encode and then decode\n",
    "encoded_text = tokenizer.encode(original_text)\n",
    "decoded_text = tokenizer.decode(encoded_text)\n",
    "print(f\"Encoded and then decoded: {decoded_text}\")\n",
    "\n",
    "# 8. Subword tokenization example\n",
    "print(\"\\nSubword tokenization examples:\")\n",
    "examples = [\"unhappiness\", \"multilingual\", \"pretrained\"]\n",
    "for example in examples:\n",
    "    tokens = tokenizer.tokenize(example)\n",
    "    print(f\"'{example}' → {tokens}\")\n",
    "\n",
    "# 9. Context window demonstration with a longer text\n",
    "some_paragraphs = \" \".join([\"This is paragraph \" + str(i) +\n",
    "                           \".\" + \" It contains some text.\" * 5 for i in range(10)])\n",
    "tokens = tokenizer.tokenize(some_paragraphs)\n",
    "print(f\"\\nLong text example:\")\n",
    "print(f\"Number of characters: {len(some_paragraphs)}\")\n",
    "print(f\"Number of tokens: {len(tokens)}\")\n",
    "print(f\"First 10 tokens: {tokens[:10]}\")\n",
    "print(f\"Last 10 tokens: {tokens[-10:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f6b07a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
