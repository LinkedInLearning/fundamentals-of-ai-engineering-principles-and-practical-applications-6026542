{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution\n",
    "#### Task\n",
    "- Extract text from multiple document formats\n",
    "- Recognize document structure and organization\n",
    "- Extract and enrich documents with metadata\n",
    "- Create a searchable index of processed documents\n",
    "\n",
    "Your task is to implement key functions in the provided boilerplate code to create a robust document processing system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading document example.pdf: RetryError[<Future at 0x7b307b8d86e0 state=finished raised FileNotFoundError>]\n",
      "Directory not found: ./documents\n",
      "Processed 0 documents from directory\n",
      "Found 0 results for 'budget' in 2023 documents\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import Document\n",
    "from llama_index.core.node_parser import SimpleNodeParser, MarkdownNodeParser, HTMLNodeParser\n",
    "from llama_index.readers.file import PDFReader, DocxReader \n",
    "from llama_index.core.schema import MetadataMode\n",
    "import os\n",
    "from datetime import datetime\n",
    "import re\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "\n",
    "class DocumentProcessor:\n",
    "    \"\"\"A complete document processing pipeline\"\"\"\n",
    "    \n",
    "    def __init__(self, storage_dir: str = \"./processed_docs\"):\n",
    "        \"\"\"Initialize the document processor\"\"\"\n",
    "        self.storage_dir = storage_dir\n",
    "        self.documents = []\n",
    "        self.nodes = []\n",
    "        self.document_map = []\n",
    "        \n",
    "        # Create storage directory if it doesn't exist\n",
    "        if not os.path.exists(storage_dir):\n",
    "            os.makedirs(storage_dir)\n",
    "    \n",
    "    def load_document(self, file_path: str) -> Optional[Document]:\n",
    "        \"\"\"\n",
    "        Load a document from a file path using the appropriate reader\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Get file extension (lowercase)\n",
    "            _, ext = os.path.splitext(file_path)\n",
    "            ext = ext.lower()\n",
    "            \n",
    "            # Initialize content variable\n",
    "            content = None\n",
    "            \n",
    "            # Choose appropriate reader based on file extension\n",
    "            if ext == '.pdf':\n",
    "                reader = PDFReader()\n",
    "                docs = reader.load_data(file=file_path)\n",
    "                if docs:\n",
    "                    content = docs[0].text\n",
    "            elif ext in ['.docx', '.doc']:\n",
    "                reader = DocxReader()\n",
    "                docs = reader.load_data(file=file_path)\n",
    "                if docs:\n",
    "                    content = docs[0].text\n",
    "            elif ext in ['.txt', '.md', '.markdown']:\n",
    "                # Simple text file reading\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    content = f.read()\n",
    "            elif ext in ['.html', '.htm']:\n",
    "                # Simple HTML file reading\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    content = f.read()\n",
    "            else:\n",
    "                print(f\"Unsupported file type: {ext}\")\n",
    "                return None\n",
    "                \n",
    "            if content:\n",
    "                # Create document with basic file metadata\n",
    "                doc = Document(\n",
    "                    text=content,\n",
    "                    metadata={\n",
    "                        \"file_path\": file_path,\n",
    "                        \"file_name\": os.path.basename(file_path),\n",
    "                        \"file_type\": ext[1:]  # Remove the dot\n",
    "                    }\n",
    "                )\n",
    "                return doc\n",
    "            else:\n",
    "                print(f\"No content extracted from {file_path}\")\n",
    "                return None\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading document {file_path}: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def extract_metadata(self, doc: Document) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Extract metadata from document content and file information\n",
    "        \"\"\"\n",
    "        metadata = {}\n",
    "        \n",
    "        # Copy existing metadata\n",
    "        if doc.metadata:\n",
    "            metadata.update(doc.metadata)\n",
    "        \n",
    "        # Add basic information\n",
    "        metadata[\"file_size\"] = len(doc.text)\n",
    "        metadata[\"extracted_date\"] = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "        metadata[\"num_characters\"] = len(doc.text)\n",
    "        metadata[\"num_words\"] = len(doc.text.split())\n",
    "        \n",
    "        # Extract potential date/year using regex\n",
    "        # Look for common date formats: YYYY-MM-DD, MM/DD/YYYY, etc.\n",
    "        date_patterns = [\n",
    "            r'\\b(20\\d{2}[/\\-]\\d{1,2}[/\\-]\\d{1,2})\\b',  # 2023-01-01\n",
    "            r'\\b(\\d{1,2}[/\\-]\\d{1,2}[/\\-]20\\d{2})\\b',  # 01-01-2023\n",
    "            r'\\b(January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{1,2},\\s+20\\d{2}\\b'  # January 1, 2023\n",
    "        ]\n",
    "        \n",
    "        for pattern in date_patterns:\n",
    "            match = re.search(pattern, doc.text)\n",
    "            if match:\n",
    "                metadata[\"date_found\"] = match.group(1)\n",
    "                break\n",
    "        \n",
    "        # Extract year\n",
    "        year_pattern = r'\\b(20\\d{2})\\b'\n",
    "        year_matches = re.findall(year_pattern, doc.text)\n",
    "        if year_matches:\n",
    "            # Use the most frequent year\n",
    "            year_counts = {}\n",
    "            for year in year_matches:\n",
    "                year_counts[year] = year_counts.get(year, 0) + 1\n",
    "            \n",
    "            most_common_year = max(year_counts.items(), key=lambda x: x[1])[0]\n",
    "            metadata[\"year\"] = most_common_year\n",
    "        \n",
    "        # Try to identify document type\n",
    "        doc_type_patterns = {\n",
    "            \"report\": r'\\breport\\b',\n",
    "            \"invoice\": r'\\binvoice\\b',\n",
    "            \"proposal\": r'\\bproposal\\b',\n",
    "            \"contract\": r'\\bcontract\\b|agreement\\b',\n",
    "            \"manual\": r'\\bmanual\\b|\\bguide\\b|\\binstruction',\n",
    "            \"article\": r'\\barticle\\b',\n",
    "            \"analysis\": r'\\banalysis\\b'\n",
    "        }\n",
    "        \n",
    "        for doc_type, pattern in doc_type_patterns.items():\n",
    "            if re.search(pattern, doc.text.lower()):\n",
    "                metadata[\"document_type\"] = doc_type\n",
    "                break\n",
    "        \n",
    "        # Try to extract title from first line or filename\n",
    "        lines = doc.text.strip().split('\\n')\n",
    "        if lines and len(lines[0]) < 200:  # Assume first line might be title if not too long\n",
    "            metadata[\"title\"] = lines[0].strip()\n",
    "        else:\n",
    "            # Use filename without extension as fallback title\n",
    "            filename = metadata.get(\"file_name\", \"\")\n",
    "            if filename:\n",
    "                base_name = os.path.splitext(filename)[0]\n",
    "                metadata[\"title\"] = base_name.replace(\"_\", \" \").replace(\"-\", \" \").title()\n",
    "        \n",
    "        return metadata\n",
    "    \n",
    "    def parse_document(self, doc: Document) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Parse a document into nodes based on structure\n",
    "        \"\"\"\n",
    "        # Update document with extracted metadata\n",
    "        metadata = self.extract_metadata(doc)\n",
    "        doc.metadata = metadata\n",
    "        \n",
    "        # Choose appropriate parser based on document type\n",
    "        file_type = metadata.get(\"file_type\", \"\").lower()\n",
    "        \n",
    "        if file_type in [\"md\", \"markdown\"]:\n",
    "            parser = MarkdownNodeParser()\n",
    "        elif file_type in [\"html\", \"htm\"]:\n",
    "            parser = HTMLNodeParser()\n",
    "        else:\n",
    "            # Default to simple node parser\n",
    "            parser = SimpleNodeParser.from_defaults(chunk_size=1024, chunk_overlap=100)\n",
    "        \n",
    "        # Parse the document\n",
    "        nodes = parser.get_nodes_from_documents([doc])\n",
    "        \n",
    "        # Add document ID to each node metadata\n",
    "        doc_id = len(self.documents)\n",
    "        for i, node in enumerate(nodes):\n",
    "            node.metadata[\"doc_id\"] = doc_id\n",
    "            node.metadata[\"node_id\"] = i\n",
    "            \n",
    "            # Try to identify section headings in the text\n",
    "            lines = node.text.strip().split('\\n')\n",
    "            if lines and len(lines[0]) < 150:  # Simple heuristic for headings\n",
    "                if \"heading\" not in node.metadata:\n",
    "                    node.metadata[\"heading\"] = lines[0].strip()\n",
    "        \n",
    "        # Add document to our collection\n",
    "        self.documents.append(doc)\n",
    "        self.nodes.extend(nodes)\n",
    "        \n",
    "        return nodes\n",
    "    \n",
    "    def create_document_map(self, nodes: List[Document]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Create a document map for navigation\n",
    "        \"\"\"\n",
    "        document_map = []\n",
    "        \n",
    "        for i, node in enumerate(nodes):\n",
    "            # Get heading info\n",
    "            heading = node.metadata.get(\"heading\", \"\")\n",
    "            level = node.metadata.get(\"heading_level\", 0)\n",
    "            \n",
    "            # If no heading info is found, try to create one\n",
    "            if not heading:\n",
    "                # Extract first line as potential heading\n",
    "                lines = node.text.strip().split('\\n')\n",
    "                if lines and len(lines[0]) < 150:\n",
    "                    heading = lines[0].strip()\n",
    "                    # Try to guess the level based on appearance\n",
    "                    if lines[0].startswith('#'):\n",
    "                        level = lines[0].count('#', 0, lines[0].find(' '))\n",
    "                    elif lines[0].startswith('=='):\n",
    "                        level = 1\n",
    "                    elif lines[0].startswith('--'):\n",
    "                        level = 2\n",
    "                    else:\n",
    "                        level = 1  # Default level\n",
    "                else:\n",
    "                    # Create a generic section name\n",
    "                    heading = f\"Section {i+1}\"\n",
    "                    level = 1\n",
    "            \n",
    "            # Create indentation for hierarchical display\n",
    "            indent = \"  \" * (level - 1) if level > 0 else \"\"\n",
    "            display = f\"{indent}{heading}\"\n",
    "            \n",
    "            # Find relevant metadata for the map\n",
    "            map_item = {\n",
    "                \"heading\": heading,\n",
    "                \"level\": level,\n",
    "                \"index\": i,\n",
    "                \"doc_id\": node.metadata.get(\"doc_id\", 0),\n",
    "                \"display\": display,\n",
    "                \"text_preview\": node.text[:100] + \"...\" if len(node.text) > 100 else node.text\n",
    "            }\n",
    "            \n",
    "            document_map.append(map_item)\n",
    "        \n",
    "        # Update the global document map\n",
    "        self.document_map.extend(document_map)\n",
    "        \n",
    "        return document_map\n",
    "    \n",
    "    def process_directory(self, directory_path: str) -> int:\n",
    "        \"\"\"\n",
    "        Process all documents in a directory\n",
    "        \"\"\"\n",
    "        processed_count = 0\n",
    "        failed_count = 0\n",
    "        \n",
    "        # Check if directory exists\n",
    "        if not os.path.isdir(directory_path):\n",
    "            print(f\"Directory not found: {directory_path}\")\n",
    "            return 0\n",
    "        \n",
    "        # List of supported extensions\n",
    "        supported_extensions = ['.pdf', '.docx', '.doc', '.txt', '.md', '.markdown', '.html', '.htm']\n",
    "        \n",
    "        # Process each file\n",
    "        for filename in os.listdir(directory_path):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            \n",
    "            # Skip directories\n",
    "            if os.path.isdir(file_path):\n",
    "                continue\n",
    "                \n",
    "            # Check if file type is supported\n",
    "            _, ext = os.path.splitext(filename)\n",
    "            if ext.lower() not in supported_extensions:\n",
    "                continue\n",
    "                \n",
    "            print(f\"Processing: {filename}\")\n",
    "            \n",
    "            # Load document\n",
    "            doc = self.load_document(file_path)\n",
    "            if doc:\n",
    "                # Process document\n",
    "                nodes = self.parse_document(doc)\n",
    "                self.create_document_map(nodes)\n",
    "                processed_count += 1\n",
    "                print(f\"  Success: {len(nodes)} nodes created\")\n",
    "            else:\n",
    "                failed_count += 1\n",
    "                print(f\"  Failed to process {filename}\")\n",
    "        \n",
    "        print(f\"Directory processing completed. Processed: {processed_count}, Failed: {failed_count}\")\n",
    "        return processed_count\n",
    "    \n",
    "    def search(self, query: str, filters: Optional[Dict[str, str]] = None) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Search processed documents\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        \n",
    "        if not query and not filters:\n",
    "            return results\n",
    "            \n",
    "        query = query.lower()\n",
    "        \n",
    "        # Process each node\n",
    "        for i, node in enumerate(self.nodes):\n",
    "            match = False\n",
    "            \n",
    "            # Check query text match\n",
    "            if query and query in node.text.lower():\n",
    "                match = True\n",
    "                \n",
    "            # Check metadata filters\n",
    "            if filters and match:\n",
    "                for key, value in filters.items():\n",
    "                    if key not in node.metadata or str(node.metadata[key]) != str(value):\n",
    "                        match = False\n",
    "                        break\n",
    "            elif filters and not query:\n",
    "                # If only filters are provided (no query text)\n",
    "                match = True\n",
    "                for key, value in filters.items():\n",
    "                    if key not in node.metadata or str(node.metadata[key]) != str(value):\n",
    "                        match = False\n",
    "                        break\n",
    "            \n",
    "            if match:\n",
    "                # Find the context of the match\n",
    "                if query:\n",
    "                    index = node.text.lower().find(query)\n",
    "                    start = max(0, index - 40)\n",
    "                    end = min(len(node.text), index + len(query) + 40)\n",
    "                    context = node.text[start:end]\n",
    "                    \n",
    "                    # Highlight the match\n",
    "                    if start > 0:\n",
    "                        context = \"...\" + context\n",
    "                    if end < len(node.text):\n",
    "                        context = context + \"...\"\n",
    "                else:\n",
    "                    # No query text, use beginning of node\n",
    "                    context = node.text[:100] + \"...\" if len(node.text) > 100 else node.text\n",
    "                \n",
    "                # Create result item\n",
    "                result = {\n",
    "                    \"node_id\": i,\n",
    "                    \"doc_id\": node.metadata.get(\"doc_id\", 0),\n",
    "                    \"heading\": node.metadata.get(\"heading\", \"Section\"),\n",
    "                    \"context\": context,\n",
    "                    \"metadata\": node.metadata\n",
    "                }\n",
    "                \n",
    "                results.append(result)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_document_structure(self, doc_id: int) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get the structure of a specific document\n",
    "        \"\"\"\n",
    "        if doc_id >= len(self.documents):\n",
    "            return {\"error\": \"Document ID not found\"}\n",
    "            \n",
    "        # Find all map items for this document\n",
    "        doc_sections = [item for item in self.document_map if item.get(\"doc_id\") == doc_id]\n",
    "        \n",
    "        if not doc_sections:\n",
    "            return {\"error\": \"No sections found for document\"}\n",
    "            \n",
    "        # Get document metadata\n",
    "        doc_metadata = self.documents[doc_id].metadata.copy()\n",
    "        \n",
    "        # Create structure\n",
    "        structure = {\n",
    "            \"title\": doc_metadata.get(\"title\", f\"Document {doc_id}\"),\n",
    "            \"metadata\": doc_metadata,\n",
    "            \"sections\": []\n",
    "        }\n",
    "        \n",
    "        # Organize sections\n",
    "        for section in doc_sections:\n",
    "            section_info = {\n",
    "                \"heading\": section.get(\"heading\", \"\"),\n",
    "                \"level\": section.get(\"level\", 1),\n",
    "                \"display\": section.get(\"display\", \"\"),\n",
    "                \"node_id\": section.get(\"index\")\n",
    "            }\n",
    "            structure[\"sections\"].append(section_info)\n",
    "            \n",
    "        return structure\n",
    "        \n",
    "    def save_processed_documents(self) -> bool:\n",
    "        \"\"\"\n",
    "        Save processed documents to storage directory\n",
    "        \n",
    "        This function is implemented for you\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Simple file-based storage\n",
    "            with open(f\"{self.storage_dir}/document_map.txt\", \"w\") as f:\n",
    "                for item in self.document_map:\n",
    "                    f.write(f\"{item['display']} - ID: {item['index']}\\n\")\n",
    "            \n",
    "            print(f\"Saved document map with {len(self.document_map)} entries\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving documents: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    processor = DocumentProcessor()\n",
    "    \n",
    "    # Process a single document\n",
    "    doc = processor.load_document(\"example.pdf\")\n",
    "    if doc:\n",
    "        metadata = processor.extract_metadata(doc)\n",
    "        nodes = processor.parse_document(doc)\n",
    "        doc_map = processor.create_document_map(nodes)\n",
    "        \n",
    "        print(f\"Processed document with {len(nodes)} nodes\")\n",
    "        print(f\"Document structure has {len(doc_map)} sections\")\n",
    "        \n",
    "    # Process a directory\n",
    "    num_processed = processor.process_directory(\"./documents\")\n",
    "    print(f\"Processed {num_processed} documents from directory\")\n",
    "    \n",
    "    # Search example\n",
    "    results = processor.search(\"budget\", filters={\"year\": \"2023\"})\n",
    "    print(f\"Found {len(results)} results for 'budget' in 2023 documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
