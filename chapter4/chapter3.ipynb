{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Embeddings Models\n",
    "\n",
    "In this video, we'll explore different embedding models available in the Sentence Transformers ecosystem and understand their trade-offs.\n",
    "\n",
    "Embedding models differ in many ways, including:\n",
    "1. Size and computational requirements\n",
    "2. Language support (monolingual vs. multilingual)\n",
    "3. Context length limitations\n",
    "4. Embedding dimensionality \n",
    "5. Performance on specific tasks\n",
    "\n",
    "Sentence Transformers provides access to a wide range of pre-trained models optimized for different use cases. Some popular models include:\n",
    "\n",
    "- all-MiniLM-L6-v2: A compact, efficient model that produces 384-dimensional embeddings\n",
    "- all-mpnet-base-v2: A more powerful model with 768-dimensional embeddings\n",
    "- paraphrase-multilingual-MiniLM-L12-v2: Supports 50+ languages\n",
    "- multi-qa-mpnet-base-dot-v1: Optimized for question-answering tasks\n",
    "\n",
    "When selecting an embedding model for your application, consider these factors:\n",
    "\n",
    "1. Accuracy requirements: More powerful models typically produce better embeddings but require more computation\n",
    "2. Inference speed: Smaller models are faster but may sacrifice some accuracy\n",
    "3. Resource constraints: Model size affects memory usage and deployment options\n",
    "4. Multilingual needs: Some models support multiple languages, while others are optimized for English\n",
    "5. Specific tasks: Models fine-tuned for particular tasks often perform better on those tasks\n",
    "\n",
    "Let's compare several embedding models to understand their trade-offs in a real-world production context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sentence_transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrandom\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer, util\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtime\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'sentence_transformers'"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# Set a nice visual style\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams.update({'font.size': 11})\n",
    "\n",
    "# Different embeddings models\n",
    "models = [\n",
    "    'all-MiniLM-L6-v2',  # Small model (384d)\n",
    "    'all-MiniLM-L12-v2',  # Medium model (384d)\n",
    "    'all-mpnet-base-v2',  # Large model (768d)\n",
    "    'paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual model (384d)\n",
    "]\n",
    "\n",
    "# Example sentences for benchmarking, organized by topic pairs\n",
    "sentence_pairs = [\n",
    "    # Technology pairs (semantically similar)\n",
    "    [\"Machine learning models require significant computational resources.\",\n",
    "     \"AI systems need a lot of computing power to train.\"],\n",
    "\n",
    "    # Programming pairs (semantically similar)\n",
    "    [\"What's the best algorithm for text classification?\",\n",
    "     \"How can I optimize my neural network training time?\"],\n",
    "\n",
    "    # Weather pairs (semantically similar)\n",
    "    [\"The weather forecast predicts rain tomorrow.\",\n",
    "     \"It's going to be wet outside tomorrow according to meteorologists.\"],\n",
    "\n",
    "    # Office pairs (semantically similar)\n",
    "    [\"I need to get a new computer for my office.\",\n",
    "     \"My workplace needs updated computing equipment.\"],\n",
    "\n",
    "    # Health pairs (semantically similar)\n",
    "    [\"Regular exercise improves cardiovascular health.\",\n",
    "     \"Working out frequently is good for your heart.\"],\n",
    "\n",
    "    # Food pairs (semantically similar)\n",
    "    [\"The restaurant serves authentic Italian pasta dishes.\",\n",
    "     \"You can get genuine Italian noodle recipes at that dining place.\"]\n",
    "]\n",
    "\n",
    "# Generate dissimilar pairs by mixing topics\n",
    "dissimilar_pairs = []\n",
    "for i in range(len(sentence_pairs)):\n",
    "    for j in range(len(sentence_pairs)):\n",
    "        if i != j:  # Different topics\n",
    "            dissimilar_pairs.append(\n",
    "                [sentence_pairs[i][0], sentence_pairs[j][0]])\n",
    "\n",
    "# Just use 6 dissimilar pairs to match our similar pairs count\n",
    "random.seed(42)  # For reproducibility\n",
    "dissimilar_pairs = random.sample(dissimilar_pairs, 6)\n",
    "\n",
    "# Create flat list of all sentences for encoding\n",
    "all_sentences = []\n",
    "for pair in sentence_pairs + dissimilar_pairs:\n",
    "    all_sentences.extend(pair)\n",
    "all_sentences = list(set(all_sentences))  # Remove duplicates\n",
    "\n",
    "results = []\n",
    "\n",
    "# Evaluation loop\n",
    "for model_name in models:\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Load model\n",
    "    model_load_time = time.time()\n",
    "    model = SentenceTransformer(model_name)\n",
    "    model_load_time = time.time() - model_load_time\n",
    "\n",
    "    # Encode all sentences at once (more efficient)\n",
    "    encoding_time = time.time()\n",
    "    embeddings_dict = {sentence: model.encode(\n",
    "        sentence) for sentence in all_sentences}\n",
    "    encoding_time = time.time() - encoding_time\n",
    "\n",
    "    # Calculate embedding dimensionality\n",
    "    dim = next(iter(embeddings_dict.values())).shape[0]\n",
    "\n",
    "    # Get model size (parameters)\n",
    "    model_size_mb = sum(p.numel() for p in model.parameters()\n",
    "                        ) * 4 / 1024 / 1024  # Approx size in MB\n",
    "\n",
    "    # Calculate similarities for similar pairs\n",
    "    similar_scores = []\n",
    "    for s1, s2 in sentence_pairs:\n",
    "        score = util.cos_sim(\n",
    "            embeddings_dict[s1].reshape(1, -1),\n",
    "            embeddings_dict[s2].reshape(1, -1)\n",
    "        ).item()\n",
    "        similar_scores.append(score)\n",
    "\n",
    "    # Calculate similarities for dissimilar pairs\n",
    "    dissimilar_scores = []\n",
    "    for s1, s2 in dissimilar_pairs:\n",
    "        score = util.cos_sim(\n",
    "            embeddings_dict[s1].reshape(1, -1),\n",
    "            embeddings_dict[s2].reshape(1, -1)\n",
    "        ).item()\n",
    "        dissimilar_scores.append(score)\n",
    "\n",
    "    # Calculate average scores and contrast\n",
    "    avg_similar = sum(similar_scores) / len(similar_scores)\n",
    "    avg_dissimilar = sum(dissimilar_scores) / len(dissimilar_scores)\n",
    "    contrast = avg_similar - avg_dissimilar\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "\n",
    "    # Store results\n",
    "    results.append({\n",
    "        'Model': model_name,\n",
    "        'Dimensions': dim,\n",
    "        'Size (MB)': model_size_mb,\n",
    "        'Load Time (s)': model_load_time,\n",
    "        'Encoding Time (s)': encoding_time,\n",
    "        'Total Time (s)': total_time,\n",
    "        'Avg Similar Score': avg_similar,\n",
    "        'Avg Different Score': avg_dissimilar,\n",
    "        'Contrast': contrast,\n",
    "        'Similar Scores': similar_scores,\n",
    "        'Dissimilar Scores': dissimilar_scores\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame for easier analysis\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Create a more readable version of the results for display\n",
    "display_df = df[['Model', 'Dimensions', 'Size (MB)', 'Encoding Time (s)',\n",
    "                'Avg Similar Score', 'Avg Different Score', 'Contrast']]\n",
    "print(\"\\n=== Model Comparison Summary ===\")\n",
    "print(display_df.to_string(index=False, float_format=lambda x: f\"{x:.4f}\"))\n",
    "\n",
    "# Detailed pair analysis\n",
    "print(\"\\n=== Detailed Pair Analysis ===\")\n",
    "for idx, model_data in enumerate(results):\n",
    "    print(f\"\\nModel: {model_data['Model']}\")\n",
    "    print(\"Similar pairs:\")\n",
    "    for i, score in enumerate(model_data['Similar Scores']):\n",
    "        print(\n",
    "            f\"  - Pair {i+1}: {sentence_pairs[i][0]} | {sentence_pairs[i][1]} | Score: {score:.4f}\")\n",
    "    print(\"Dissimilar pairs:\")\n",
    "    for i, score in enumerate(model_data['Dissimilar Scores']):\n",
    "        print(\n",
    "            f\"  - Pair {i+1}: {dissimilar_pairs[i][0]} | {dissimilar_pairs[i][1]} | Score: {score:.4f}\")\n",
    "\n",
    "# Visualize model comparison with improved plots\n",
    "plt.figure(figsize=(20, 16))\n",
    "\n",
    "# Plot 1: Model Performance Dashboard\n",
    "plt.subplot(2, 2, 1)\n",
    "# Create x-axis with model names\n",
    "x = np.arange(len(models))\n",
    "width = 0.25\n",
    "\n",
    "# Plot bars for each metric\n",
    "plt.bar(x - width, df['Avg Similar Score'], width,\n",
    "        label='Avg Similar Score', color='green', alpha=0.7)\n",
    "plt.bar(x, df['Avg Different Score'], width,\n",
    "        label='Avg Different Score', color='red', alpha=0.7)\n",
    "plt.bar(x + width, df['Contrast'], width,\n",
    "        label='Contrast', color='blue', alpha=0.7)\n",
    "\n",
    "plt.xticks(x, df['Model'], rotation=45, ha='right')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.legend()\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Plot 2: Similar vs Different distribution\n",
    "plt.subplot(2, 2, 2)\n",
    "\n",
    "# Prepare data for boxplot\n",
    "boxplot_data = []\n",
    "model_names = []\n",
    "\n",
    "for model_data in results:\n",
    "    boxplot_data.append(model_data['Similar Scores'])\n",
    "    boxplot_data.append(model_data['Dissimilar Scores'])\n",
    "    model_names.append(f\"{model_data['Model']} Similar\")\n",
    "    model_names.append(f\"{model_data['Model']} Different\")\n",
    "\n",
    "# Create a color map for the boxes\n",
    "colors = ['lightgreen', 'lightcoral'] * len(models)\n",
    "\n",
    "# Create boxplot\n",
    "bp = plt.boxplot(boxplot_data, patch_artist=True, vert=False)\n",
    "plt.yticks(range(1, len(model_names) + 1), model_names)\n",
    "plt.xlabel('Cosine Similarity')\n",
    "plt.title('Distribution of Similarity Scores by Model')\n",
    "\n",
    "# Color the boxes\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "\n",
    "# Plot 3: Efficiency metrics - bubble chart\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.scatter(df['Size (MB)'], df['Encoding Time (s)'],\n",
    "            s=df['Dimensions']/5, alpha=0.7)\n",
    "for i, model in enumerate(df['Model']):\n",
    "    plt.annotate(model, (df['Size (MB)'].iloc[i], df['Encoding Time (s)'].iloc[i]),\n",
    "                 xytext=(5, 5), textcoords='offset points')\n",
    "plt.xlabel('Model Size (MB)')\n",
    "plt.ylabel('Encoding Time (s)')\n",
    "plt.title('Model Efficiency (bubble size = dimensions)')\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot 4: Performance vs Resource Requirements\n",
    "plt.subplot(2, 2, 4)\n",
    "sc = plt.scatter(df['Total Time (s)'], df['Contrast'], c=df['Size (MB)'],\n",
    "                 s=200, cmap='viridis', alpha=0.7)\n",
    "plt.colorbar(sc, label='Model Size (MB)')\n",
    "\n",
    "for i, model in enumerate(df['Model']):\n",
    "    plt.annotate(model, (df['Total Time (s)'].iloc[i], df['Contrast'].iloc[i]),\n",
    "                 xytext=(5, 5), textcoords='offset points')\n",
    "plt.xlabel('Total Processing Time (s)')\n",
    "plt.ylabel('Contrast (Similar - Different)')\n",
    "plt.title('Performance vs. Resource Requirements')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout(pad=3.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Takeaways\n",
    "\n",
    "- Contrast is king: The contrast score is often the most important metric. A model with higher contrast will be better at distinguishing relevant from irrelevant information.\n",
    "- Consider the trade-offs: Larger models typically have better semantic understanding but require more resources and run slower. Ask yourself what's more important for your application: accuracy or speed?\n",
    "- Test with domain-specific data: While our comparison uses general examples, you'll get the best results by testing with examples from your specific domain.\n",
    "- Think about your constraints: If you're deploying on mobile or edge devices, model size matters more. For server applications, you might prioritize accuracy.\n",
    "- Consider multilingual needs: If your application needs to work across languages, a multilingual model might be essential even if it's not the top performer in English.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
